#!/usr/bin/env python3
"""
vLLM Inference Script for Vulnerability Detection Pipeline
Separated inference component that saves raw model outputs

This file is maintained for backward compatibility.
The main implementation is now in inference_engines/vllm_engine.py
"""

import argparse
import logging

# Re-export from new location for backward compatibility
from inference_engines.vllm_engine import VLLMInferenceEngine
from inference_engines.base import InferenceResult

# Keep original exports for direct usage
__all__ = ['VLLMInferenceEngine', 'InferenceResult']


def main():
    """Main function for CLI usage - maintained for backward compatibility"""
    parser = argparse.ArgumentParser(description="Run vLLM inference for vulnerability detection")
    parser.add_argument("--model_path", type=str, required=True, help="Path to the model")
    parser.add_argument("--data_path", type=str, required=True, help="Path to the data file (JSONL)")
    parser.add_argument("--output_dir", type=str, default="./inference_results", help="Output directory for results")
    parser.add_argument("--output_model_name", type=str, default=None, help="Explicit name for the model in output files (default: auto-generated from model_path)")
    parser.add_argument("--temperature", type=float, default=0.0, help="Sampling temperature")
    parser.add_argument("--max_tokens", type=int, default=4096, help="Maximum tokens to generate")
    parser.add_argument("--top_p", type=float, default=None, help="Top-p sampling parameter")
    parser.add_argument("--top_k", type=int, default=None, help="Top-k sampling parameter")
    parser.add_argument("--max_samples", type=int, default=None, help="Maximum number of samples to process")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    parser.add_argument("--tensor_parallel_size", type=int, default=1, help="Number of GPUs for tensor parallelism")
    parser.add_argument("--gpu_memory_utilization", type=float, default=0.8, help="GPU memory utilization ratio")
    parser.add_argument("--format_prompt", default=True, action="store_true", help="Use chat template formatting")
    parser.add_argument("--system_prompt", type=str, default=None, help="Path to text file containing system prompt")
    parser.add_argument("--no_balanced_sampling", action="store_true", help="Disable balanced sampling")
    parser.add_argument("--prompt_column", type=str, default=None, help="Column name containing the prompt text")
    parser.add_argument("--max_model_len", type=int, default=None, help="Maximum model context length (tokens). If not specified, auto-detected from model config.")
    parser.add_argument("--max_prompt_tokens", type=int, default=None, help="Maximum prompt length before truncation. If not specified, auto-calculated based on model context length.")
    parser.add_argument("--template", type=str, required=True, choices=["think", "direct", "direct_new","r2vul","think_not_tune","think_with_cwe","think_with_cwe_new"], help="Template type to use for prompt formatting")
    parser.add_argument("--num_samples", type=int, default=1, help="Number of samples to generate per prompt for majority voting")

    args = parser.parse_args()

    # Initialize inference engine
    engine = VLLMInferenceEngine(
        model_path=args.model_path,
        tensor_parallel_size=args.tensor_parallel_size,
        gpu_memory_utilization=args.gpu_memory_utilization,
        max_model_len=args.max_model_len,
        max_prompt_tokens=args.max_prompt_tokens,
        template=args.template,
        system_prompt_file=args.system_prompt,
        output_model_name=args.output_model_name
    )

    # Run inference
    result = engine.run_inference_pipeline(
        data_path=args.data_path,
        output_dir=args.output_dir,
        temperature=args.temperature,
        max_tokens=args.max_tokens,
        top_p=args.top_p,
        top_k=args.top_k,
        max_samples=args.max_samples,
        seed=args.seed,
        format_prompt=args.format_prompt,
        balanced_sampling=not args.no_balanced_sampling,
        prompt_column=args.prompt_column,
        num_samples=args.num_samples
    )

    print(f"Inference completed successfully!")
    print(f"Processed {len(result.responses)} samples")
    print(f"Results saved to {args.output_dir}")


if __name__ == "__main__":
    main()
