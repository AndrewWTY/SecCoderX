"""
Inference Engines Package
Provides modular backends for vulnerability detection inference
"""

from .base import BaseInferenceEngine, InferenceResult

def get_engine(backend: str, **kwargs):
    """
    Factory function to get the appropriate inference engine.

    Args:
        backend: One of 'vllm', 'openai', 'gemini', 'codeql', 'bandit', 'semgrep'
        **kwargs: Backend-specific configuration

    Returns:
        BaseInferenceEngine instance
    """
    backend = backend.lower()

    if backend == 'vllm':
        from .vllm_engine import VLLMInferenceEngine
        return VLLMInferenceEngine(**kwargs)
    elif backend == 'openai':
        from .openai_engine import OpenAIBatchEngine
        return OpenAIBatchEngine(**kwargs)
    elif backend == 'gemini':
        from .gemini_engine import GeminiBatchEngine
        return GeminiBatchEngine(**kwargs)
    elif backend == 'codeql':
        from .codeql_engine import CodeQLEngine
        return CodeQLEngine(**kwargs)
    elif backend == 'bandit':
        from .bandit_engine import BanditEngine
        return BanditEngine(**kwargs)
    elif backend == 'semgrep':
        from .semgrep_engine import SemgrepEngine
        return SemgrepEngine(**kwargs)
    else:
        raise ValueError(f"Unknown backend: {backend}. Choose from: vllm, openai, gemini, codeql, bandit, semgrep")

__all__ = ['BaseInferenceEngine', 'InferenceResult', 'get_engine']
