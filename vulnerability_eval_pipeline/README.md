# Vulnerability Detection Evaluation Pipeline using vLLM

A comprehensive evaluation pipeline for assessing vulnerability detection capabilities of language models using vLLM for efficient inference.

## Features

- **vLLM Integration**: High-performance inference using vLLM for fast batch processing
- **Flexible Prompt Templates**: Customizable prompt templates for different evaluation scenarios
- **Comprehensive Metrics**: Accuracy, precision, recall, F1-score, and confusion matrix
- **Multi-language Support**: Evaluation across different programming languages (C, Python, JavaScript, etc.)
- **CWE Analysis**: Detailed analysis by Common Weakness Enumeration (CWE) types
- **Robust Response Parsing**: Intelligent extraction of YES/NO predictions from model responses
- **Detailed Reporting**: Both summary metrics and individual prediction results

## Installation

1. Install required dependencies:
```bash
pip install -r requirements.txt
```

2. Ensure you have access to a compatible GPU for vLLM inference.

## Data Format

The pipeline expects data in JSONL format with the following structure:
```json
{
  "index": 1922,
  "cwe": "CWE-200",
  "cve": "CVE-2015-7665",
  "is_vulnerable": false,
  "language_suffix": "c",
  "language_name": "C",
  "code": "int main() { ... }",
  "dataset": "primevul",
  "dataset_index": 2627,
  "cwe_name": "Exposure of Sensitive Information",
  "cwe_desc": "The product exposes sensitive information..."
}
```

Key fields:
- `code`: The source code snippet to analyze
- `is_vulnerable`: Ground truth label (boolean)
- `language_name`: Programming language
- `cwe`: CWE identifier (optional)

## Usage

### Basic Usage

```bash
python vllm_vulnerability_eval.py \
  --model_path /path/to/your/model \
  --data_path dataset_code_sec/test_set_1_2.jsonl \
  --output_dir ./eval_results \
  --max_samples 100 \
  --temperature 0.0 \
  --max_tokens 256
```

### Command Line Arguments

- `--model_path`: Path to the model checkpoint (required)
- `--data_path`: Path to the evaluation data in JSONL format (required)
- `--output_dir`: Directory to save results (default: ./eval_results)
- `--max_samples`: Maximum number of samples to evaluate (default: all)
- `--temperature`: Sampling temperature (default: 0.0)
- `--max_tokens`: Maximum tokens to generate (default: 256)
- `--top_p`: Top-p sampling parameter (default: 1.0)
- `--tensor_parallel_size`: Number of GPUs for tensor parallelism (default: 1)
- `--gpu_memory_utilization`: GPU memory utilization ratio (default: 0.9)

### Advanced Usage

#### Multi-GPU Evaluation
```bash
python vllm_vulnerability_eval.py \
  --model_path /path/to/your/model \
  --data_path dataset_code_sec/test_set_1_2.jsonl \
  --tensor_parallel_size 2 \
  --gpu_memory_utilization 0.8
```

#### Sampling with Temperature
```bash
python vllm_vulnerability_eval.py \
  --model_path /path/to/your/model \
  --data_path dataset_code_sec/test_set_1_2.jsonl \
  --temperature 0.7 \
  --top_p 0.9
```

## Prompt Template

The pipeline uses the following prompt template by default:

```
You are a code security expert specialized in identifying and analyzing vulnerabilities in source code. Your task is to determine whether the given code snippet contains Common Weakness Enumeration (CWE) vulnerability or not.

### Code To Be Examine
```
{code}
```

Output your decision in either [[YES]] (vulnerable) or [[NO]] (not vulnerable).
```

### Custom Prompt Templates

You can customize the prompt template programmatically:

```python
from vllm_vulnerability_eval import VulnerabilityEvaluator

evaluator = VulnerabilityEvaluator(model_path="/path/to/model")

# Custom prompt
custom_prompt = """
Analyze this code for security vulnerabilities:
{code}

Answer: [[YES]] or [[NO]]
"""

evaluator.prompt_template = custom_prompt
```

## Output Files

The pipeline generates several output files:

1. **`{model_name}_evaluation_results.json`**: Overall metrics and analysis
2. **`{model_name}_detailed_results.jsonl`**: Individual predictions with metadata

### Example Results Structure

```json
{
  "overall_metrics": {
    "accuracy": 0.8500,
    "precision": 0.7800,
    "recall": 0.8200,
    "f1": 0.8000,
    "total_samples": 1000
  },
  "metrics_by_language": {
    "C": {
      "accuracy": 0.8600,
      "precision": 0.8100,
      "recall": 0.8400,
      "f1": 0.8250,
      "count": 400
    },
    "Python": {
      "accuracy": 0.8400,
      "precision": 0.7600,
      "recall": 0.8000,
      "f1": 0.7800,
      "count": 350
    }
  },
  "metrics_by_cwe": {
    "CWE-79": {
      "accuracy": 0.9000,
      "precision": 0.8800,
      "recall": 0.9100,
      "f1": 0.8950,
      "count": 50
    }
  }
}
```

## Response Parsing

The pipeline uses intelligent response parsing to extract predictions:

1. **Primary Pattern**: Looks for `[[YES]]` or `[[NO]]` in the response
2. **Fallback Pattern**: Searches for "YES" or "NO" anywhere in the response
3. **Default**: If no clear pattern is found, defaults to "not vulnerable" (False)

## Examples

### Basic Evaluation

```python
from vllm_vulnerability_eval import VulnerabilityEvaluator

evaluator = VulnerabilityEvaluator(
    model_path="meta-llama/Llama-2-7b-chat-hf",
    tensor_parallel_size=1
)

result = evaluator.evaluate(
    data_path="dataset_code_sec/test_set_1_2.jsonl",
    max_samples=100
)

print(f"Accuracy: {result.accuracy:.4f}")
print(f"F1 Score: {result.f1:.4f}")
```

### Language-Specific Analysis

```python
# After running evaluation
language_metrics = evaluator.analyze_by_language("data.jsonl", result)

for lang, metrics in language_metrics.items():
    print(f"{lang}: F1={metrics['f1']:.4f} ({metrics['count']} samples)")
```

### CWE-Specific Analysis

```python
# After running evaluation
cwe_metrics = evaluator.analyze_by_cwe("data.jsonl", result)

for cwe, metrics in cwe_metrics.items():
    if metrics['count'] >= 10:  # Only show CWEs with sufficient samples
        print(f"{cwe}: F1={metrics['f1']:.4f} ({metrics['count']} samples)")
```

## Performance Considerations

- **GPU Memory**: Adjust `gpu_memory_utilization` based on your GPU capacity
- **Batch Size**: vLLM automatically handles batching for optimal performance
- **Tensor Parallelism**: Use multiple GPUs for large models
- **Sampling**: Use `temperature=0.0` for deterministic results

## Troubleshooting

### Common Issues

1. **CUDA Out of Memory**: Reduce `gpu_memory_utilization` or use tensor parallelism
2. **Model Loading Errors**: Ensure the model path is correct and accessible
3. **Response Parsing Issues**: Check model responses in detailed output file
4. **Data Format Errors**: Verify JSONL format and required fields

### Debug Mode

Enable verbose logging for debugging:

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## Model Compatibility

The pipeline is compatible with:
- **Local Models**: Any model supported by vLLM
- **HuggingFace Models**: Direct loading from HuggingFace Hub
- **Code-Specific Models**: CodeLlama, StarCoder, etc.
- **Chat Models**: Llama-2-Chat, Vicuna, etc.

## Contributing

Feel free to submit issues and enhancement requests. Contributions are welcome!

## License

This project is licensed under the MIT License. 