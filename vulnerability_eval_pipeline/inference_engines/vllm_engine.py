"""
vLLM Inference Engine - For local/HuggingFace models using vLLM
"""

import json
import os
from typing import List, Dict, Any

from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest
from transformers import AutoTokenizer, AutoConfig

from .base import BaseInferenceEngine, InferenceResult


class VLLMInferenceEngine(BaseInferenceEngine):
    """Inference engine using vLLM for local models"""

    def __init__(self, model_path: str, tensor_parallel_size: int = 1,
                 gpu_memory_utilization: float = 0.8, max_model_len: int = None,
                 max_prompt_tokens: int = None, template: str = None,
                 system_prompt_file: str = None, output_model_name: str = None):
        """
        Initialize the vLLM inference engine.

        Args:
            model_path: Path to the model or HuggingFace model ID
            tensor_parallel_size: Number of GPUs for tensor parallelism
            gpu_memory_utilization: GPU memory utilization ratio
            max_model_len: Maximum model context length
            max_prompt_tokens: Maximum prompt length before truncation
            template: Template type or path
            system_prompt_file: Path to system prompt file
            output_model_name: Name to use in output files
        """
        self.model_path = model_path
        self.tensor_parallel_size = tensor_parallel_size
        self.gpu_memory_utilization = gpu_memory_utilization

        # Load tokenizer first (needed for parent class operations)
        self.tokenizer = None
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        except Exception as e:
            pass  # Will try again after LoRA detection

        # Detect LoRA adapter
        self.is_lora_adapter, self.base_model_path = self._detect_lora_adapter(model_path)

        if self.is_lora_adapter:
            self.lora_adapter_path = model_path
            if self.tokenizer is None:
                try:
                    self.tokenizer = AutoTokenizer.from_pretrained(self.lora_adapter_path, trust_remote_code=True)
                except Exception:
                    try:
                        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_path, trust_remote_code=True)
                    except Exception:
                        pass
        else:
            self.lora_adapter_path = None
            self.base_model_path = model_path

        # Initialize parent class
        super().__init__(
            template=template,
            system_prompt_file=system_prompt_file,
            output_model_name=output_model_name,
            max_prompt_tokens=max_prompt_tokens
        )

        # Auto-detect model context length
        self.model_max_length = self._detect_model_context_length()
        self.logger.info(f"Detected model context length: {self.model_max_length}")

        # Set max_model_len with intelligent defaults
        if max_model_len is None:
            self.max_model_len = self.model_max_length
        else:
            self.max_model_len = min(max_model_len, self.model_max_length)

        if max_prompt_tokens is None:
            generation_buffer = min(2048, self.max_model_len // 4)
            self.max_prompt_tokens = self.max_model_len - generation_buffer
        else:
            generation_buffer = min(2048, self.max_model_len // 4)
            self.max_prompt_tokens = min(max_prompt_tokens, self.max_model_len - generation_buffer)

        self.logger.info(f"Using max_model_len: {self.max_model_len}")
        self.logger.info(f"Using max_prompt_tokens: {self.max_prompt_tokens}")

        # Initialize vLLM model
        self.llm = LLM(
            model=self.base_model_path,
            tensor_parallel_size=tensor_parallel_size,
            gpu_memory_utilization=gpu_memory_utilization,
            trust_remote_code=True,
            max_model_len=self.max_model_len,
            enable_lora=self.is_lora_adapter,
        )

    def _detect_model_context_length(self) -> int:
        """Detect the model's maximum context length"""
        try:
            context_length = None

            # Method 1: Load model config directly
            try:
                config = AutoConfig.from_pretrained(self.base_model_path, trust_remote_code=True)
                for attr in ['max_position_embeddings', 'n_positions', 'seq_length', 'max_seq_length']:
                    if hasattr(config, attr):
                        value = getattr(config, attr)
                        if value and isinstance(value, int) and value > 0:
                            context_length = value
                            self.logger.info(f"Found context length from config.{attr}: {context_length}")
                            break
            except Exception as e:
                self.logger.warning(f"Could not load model config: {e}")

            # Method 2: Check tokenizer model_max_length
            if context_length is None and self.tokenizer and hasattr(self.tokenizer, 'model_max_length'):
                if self.tokenizer.model_max_length != float('inf'):
                    context_length = self.tokenizer.model_max_length
                    self.logger.info(f"Found context length from tokenizer.model_max_length: {context_length}")

            # Method 3: Known model patterns
            if context_length is None:
                model_name_lower = self.base_model_path.lower()
                if 'qwen2.5' in model_name_lower or 'qwen' in model_name_lower:
                    context_length = 32768
                    self.logger.info(f"Using known context length for Qwen: {context_length}")
                elif 'llama' in model_name_lower:
                    context_length = 4096
                    self.logger.info(f"Using known context length for Llama: {context_length}")

            # Fallback
            if context_length is None:
                context_length = 30000
                self.logger.warning(f"Could not detect context length, using default: {context_length}")

            return context_length

        except Exception as e:
            self.logger.warning(f"Error detecting context length: {e}, using default 30000")
            return 30000

    def _detect_lora_adapter(self, model_path: str) -> tuple:
        """Detect if the model_path is a LoRA adapter and get the base model path."""
        adapter_config_path = os.path.join(model_path, "adapter_config.json")

        if os.path.exists(adapter_config_path):
            try:
                with open(adapter_config_path, 'r', encoding='utf-8') as f:
                    adapter_config = json.load(f)

                base_model_path = adapter_config.get("base_model_name_or_path")
                if base_model_path:
                    self.logger.info(f"Detected LoRA adapter at {model_path}")
                    self.logger.info(f"Base model: {base_model_path}")
                    return True, base_model_path
                else:
                    raise ValueError(f"LoRA adapter config found but 'base_model_name_or_path' is missing")

            except json.JSONDecodeError as e:
                raise ValueError(f"Invalid JSON in adapter config: {e}")
            except Exception as e:
                raise ValueError(f"Error reading adapter config: {e}")

        return False, model_path

    def truncate_prompt(self, prompt: str, max_tokens: int = None) -> str:
        """Truncate prompt if it exceeds max_tokens when tokenized"""
        if self.tokenizer is None:
            return prompt

        max_tokens = max_tokens or self.max_prompt_tokens

        try:
            tokens = self.tokenizer.encode(prompt)

            if len(tokens) > max_tokens:
                self.truncated_prompts_count += 1
                self.logger.warning(f"Prompt too long ({len(tokens)} tokens), truncating to {max_tokens} tokens")

                instruction_tokens = 200
                context_tokens = max_tokens - instruction_tokens

                if len(tokens) > instruction_tokens:
                    truncated_tokens = tokens[:context_tokens] + tokens[-instruction_tokens:]
                else:
                    truncated_tokens = tokens[:max_tokens]

                prompt = self.tokenizer.decode(truncated_tokens, skip_special_tokens=True)

                if "### Code To Be Examined" in prompt or "vulnerability" in prompt.lower():
                    prompt = prompt.replace("### Code To Be Examined", "[Content truncated due to length]\n\n### Code To Be Examined")

            return prompt
        except Exception as e:
            self.logger.warning(f"Error during tokenization/truncation: {e}")
            return prompt

    def _format_prompt(self, input_text: str, no_system: bool = False) -> str:
        """Format prompt using chat template"""
        if no_system:
            chat = [{"role": "user", "content": self.system_prompt + input_text}]
        else:
            if self.system_prompt:
                chat = [
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": input_text},
                ]
            else:
                chat = [{"role": "user", "content": input_text}]

        if self.tokenizer.chat_template is None:
            raise ValueError("The tokenizer does not have a chat template.")
        formatted_input = self.tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
        return formatted_input

    def prepare_prompts(self, data: List[Dict[str, Any]], format_prompt: bool = False,
                        prompt_column: str = None) -> List[str]:
        """Prepare prompts for inference with optional chat template formatting"""
        # Use parent class to get base prompts
        prompts = self.prepare_prompts_from_data(data, prompt_column)

        # Apply truncation and optional formatting
        processed_prompts = []
        for prompt in prompts:
            prompt = self.truncate_prompt(prompt, self.max_prompt_tokens)

            if format_prompt and self.tokenizer:
                print('Applying chat template')
                prompt = self._format_prompt(prompt, no_system=False)

            processed_prompts.append(prompt)

        return processed_prompts

    def run_inference(self, prompts: List[str], temperature: float = 0.0,
                      max_tokens: int = 4096, top_p: float = None,
                      top_k: int = None, seed: int = 42,
                      num_samples: int = 1) -> List[List[str]]:
        """Run inference using vLLM"""
        # Define stop tokens
        stop_tokens = []
        if self.tokenizer:
            if self.tokenizer.eos_token:
                stop_tokens.append(self.tokenizer.eos_token)
        stop_tokens.append("<|im_end|>")

        print('Stop tokens:', stop_tokens)

        # Build sampling params
        sampling_params_dict = {
            'temperature': temperature,
            'max_tokens': max_tokens,
            'stop': stop_tokens if stop_tokens else None,
            'seed': seed,
            'skip_special_tokens': False,
            'include_stop_str_in_output': True,
            'n': num_samples,
        }

        if top_p is not None:
            sampling_params_dict['top_p'] = top_p
        if top_k is not None:
            sampling_params_dict['top_k'] = top_k

        sampling_params = SamplingParams(**sampling_params_dict)
        self.logger.info(f"Running inference on {len(prompts)} samples with {num_samples} samples per prompt")

        # Use LoRARequest if this is a LoRA adapter
        if self.is_lora_adapter:
            lora_request = LoRARequest("lora_adapter", 1, self.lora_adapter_path)
            self.logger.info(f"Using LoRA adapter: {self.lora_adapter_path}")
            outputs = self.llm.generate(prompts, sampling_params, lora_request=lora_request)
        else:
            outputs = self.llm.generate(prompts, sampling_params)

        responses = []
        for output in outputs:
            prompt_responses = []
            for sample_output in output.outputs:
                prompt_responses.append(sample_output.text)
            responses.append(prompt_responses)

        return responses

    def run_inference_pipeline(self, data_path: str, output_dir: str,
                                temperature: float = 0.0, max_tokens: int = 4096,
                                top_p: float = None, top_k: int = None,
                                max_samples: int = None, seed: int = 42,
                                format_prompt: bool = True,
                                balanced_sampling: bool = True,
                                prompt_column: str = None,
                                num_samples: int = 1) -> InferenceResult:
        """Run complete inference pipeline and save results"""
        # Extract dataset name and create output directory
        dataset_name = os.path.splitext(os.path.basename(data_path))[0]
        dataset_output_dir = os.path.join(output_dir, dataset_name)
        os.makedirs(dataset_output_dir, exist_ok=True)

        # Load data
        self.logger.info(f"Loading data from {data_path}")
        data = self.load_data(data_path)

        if max_samples:
            if balanced_sampling:
                data = self.balanced_sample(data, max_samples, seed)
                self.logger.info(f"Using balanced sample of {len(data)} samples for inference")
            else:
                data = data[:max_samples]
                self.logger.info(f"Using first {len(data)} samples for inference")

        # Prepare prompts
        prompts = self.prepare_prompts(data, format_prompt=format_prompt, prompt_column=prompt_column)

        # Run inference
        responses = self.run_inference(prompts, temperature, max_tokens, top_p, top_k, seed, num_samples)

        # Create result
        result = InferenceResult(
            prompts=prompts,
            responses=responses,
            metadata=data
        )

        # Save results
        if self.output_model_name:
            model_name = self.output_model_name
        elif self.is_lora_adapter:
            base_model_name = os.path.basename(self.base_model_path).replace('/', '_')
            lora_adapter_name = os.path.basename(self.lora_adapter_path).replace('/', '_')
            model_name = f"{base_model_name}_lora_{lora_adapter_name}"
        else:
            model_name = os.path.basename(self.model_path).replace('/', '_')

        inference_file = os.path.join(dataset_output_dir, f"{model_name}_{self.template}_{num_samples}_inference_results.jsonl")

        inference_params = {
            'temperature': temperature,
            'max_tokens': max_tokens,
            'top_p': top_p,
            'top_k': top_k,
            'seed': seed,
            'num_samples': num_samples
        }

        self.save_inference_results(prompts, responses, data, inference_file, inference_params)
        self.logger.info(f"Inference completed. Results saved to {inference_file}")

        return result
