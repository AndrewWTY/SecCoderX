"""
Gemini Batch API Inference Engine
Uses Google's Gemini Batch API for cost-effective large-scale inference (50% discount)
"""

import json
import os
import time
from datetime import datetime
from typing import List, Dict, Any, Optional
from tqdm import tqdm

from google import genai
from google.genai import types

from .base import BaseInferenceEngine, InferenceResult


class GeminiBatchEngine(BaseInferenceEngine):
    """Inference engine using Google's Gemini Batch API with file-based requests"""

    # Job state constants
    JOB_STATE_PENDING = "JOB_STATE_PENDING"
    JOB_STATE_RUNNING = "JOB_STATE_RUNNING"
    JOB_STATE_SUCCEEDED = "JOB_STATE_SUCCEEDED"
    JOB_STATE_FAILED = "JOB_STATE_FAILED"
    JOB_STATE_CANCELLED = "JOB_STATE_CANCELLED"
    JOB_STATE_EXPIRED = "JOB_STATE_EXPIRED"

    TERMINAL_STATES = {JOB_STATE_SUCCEEDED, JOB_STATE_FAILED, JOB_STATE_CANCELLED, JOB_STATE_EXPIRED}

    def __init__(self, model_name: str = "gemini-2.5-flash-pro", template: str = None,
                 system_prompt_file: str = None, output_model_name: str = None,
                 max_prompt_tokens: int = None, api_key: str = None,
                 poll_interval: int = 30, max_wait_hours: int = 24,
                 batch_requests_dir: str = None, auto_submit: bool = False):
        """
        Initialize the Gemini Batch API engine.

        Args:
            model_name: Gemini model name (e.g., 'gemini-2.5-flash-pro', 'gemini-2.5-pro')
            template: Template type or path
            system_prompt_file: Path to system prompt file
            output_model_name: Name to use in output files
            max_prompt_tokens: Maximum prompt length
            api_key: Gemini API key (or read from config/env)
            poll_interval: Seconds between status polls
            max_wait_hours: Maximum hours to wait for batch completion
            batch_requests_dir: Directory to store batch request files
            auto_submit: Skip confirmation prompt and submit batch directly
        """
        # Initialize parent class first
        super().__init__(
            template=template,
            system_prompt_file=system_prompt_file,
            output_model_name=output_model_name or model_name,
            max_prompt_tokens=max_prompt_tokens
        )

        self.model_name = model_name
        self.poll_interval = poll_interval
        self.max_wait_hours = max_wait_hours
        self.auto_submit = auto_submit

        # Set batch requests directory
        if batch_requests_dir:
            self.batch_requests_dir = batch_requests_dir
        else:
            self.batch_requests_dir = os.path.join(self.project_root, 'batch_requests', 'gemini')
        os.makedirs(self.batch_requests_dir, exist_ok=True)

        # Load API key
        if api_key:
            self._api_key = api_key
        else:
            self._api_key = self._load_api_key('gemini')

        # Initialize Gemini client
        self.client = genai.Client(api_key=self._api_key)
        self.logger.info(f"Initialized Gemini Batch Engine with model: {model_name}")

    def _create_batch_request(self, prompt: str, key: str,
                               temperature: float = None, max_tokens: int = None) -> Dict[str, Any]:
        """Create a single batch request entry in JSONL format"""
        request = {
            "key": key,
            "request": {
                "contents": [
                    {
                        "parts": [
                            {"text": prompt}
                        ]
                    }
                ]
            }
        }

        # Only add generation_config if we have parameters to set
        generation_config = {}
        if temperature is not None:
            generation_config["temperature"] = temperature
        if max_tokens is not None:
            generation_config["max_output_tokens"] = max_tokens

        if generation_config:
            request["request"]["generation_config"] = generation_config

        # Add system instruction if available
        if self.system_prompt:
            request["request"]["system_instruction"] = {
                "parts": [{"text": self.system_prompt}]
            }

        return request

    def _create_batch_file(self, prompts: List[str], temperature: float,
                           max_tokens: int, dataset_name: str = None) -> str:
        """Create JSONL batch request file and return path"""
        # Generate filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        dataset_prefix = f"{dataset_name}_" if dataset_name else ""
        filename = f"{dataset_prefix}{self.model_name}_{timestamp}_requests.jsonl"
        batch_file_path = os.path.join(self.batch_requests_dir, filename)

        with open(batch_file_path, 'w', encoding='utf-8') as f:
            for i, prompt in enumerate(prompts):
                request = self._create_batch_request(
                    prompt=prompt,
                    key=f"request-{i}",
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                f.write(json.dumps(request) + '\n')

        self.logger.info(f"Created batch file with {len(prompts)} requests: {batch_file_path}")
        return batch_file_path

    def _upload_batch_file(self, batch_file_path: str) -> Any:
        """Upload batch file to Gemini File API and return file reference"""
        self.logger.info(f"Uploading batch file to Gemini: {batch_file_path}")

        # Upload file using Gemini File API
        uploaded_file = self.client.files.upload(
            file=batch_file_path,
            config=types.UploadFileConfig(
                display_name=os.path.basename(batch_file_path),
                mime_type='jsonl'
            )
        )

        self.logger.info(f"Uploaded batch file: {uploaded_file.name}")
        return uploaded_file

    def _create_batch_job(self, uploaded_file: Any) -> Any:
        """Create batch job using uploaded file"""
        self.logger.info("Creating batch job...")

        batch_job = self.client.batches.create(
            model=f"models/{self.model_name}",
            src=uploaded_file.name,
            config={'display_name': f"vuln-eval-{datetime.now().strftime('%Y%m%d_%H%M%S')}"}
        )

        self.logger.info(f"Created batch job: {batch_job.name}")
        return batch_job

    def _poll_batch_status(self, batch_job_name: str) -> Dict[str, Any]:
        """Poll batch job status until completion"""
        self.logger.info(f"Polling batch job status (interval: {self.poll_interval}s, max wait: {self.max_wait_hours}h)...")

        max_wait_seconds = self.max_wait_hours * 3600
        start_time = time.time()

        with tqdm(desc="Waiting for batch completion", unit="poll") as pbar:
            while True:
                batch_job = self.client.batches.get(name=batch_job_name)
                state = batch_job.state.name if hasattr(batch_job.state, 'name') else str(batch_job.state)

                pbar.set_postfix(state=state)
                pbar.update(1)

                if state == self.JOB_STATE_SUCCEEDED:
                    self.logger.info("Batch job completed successfully!")
                    return {
                        'state': state,
                        'batch_job': batch_job
                    }
                elif state in self.TERMINAL_STATES:
                    error_msg = f"Batch job ended with state: {state}"
                    if hasattr(batch_job, 'error') and batch_job.error:
                        error_msg += f" - {batch_job.error}"
                    self.logger.error(error_msg)
                    raise RuntimeError(error_msg)

                elapsed = time.time() - start_time
                if elapsed > max_wait_seconds:
                    raise TimeoutError(f"Batch job did not complete within {self.max_wait_hours} hours")

                time.sleep(self.poll_interval)

    def _download_results(self, batch_job: Any, batch_file_path: str = None) -> List[Dict[str, Any]]:
        """Download and parse batch results from file"""
        self.logger.info("Downloading batch results...")

        results = []

        try:
            # Get result file from batch job destination
            if hasattr(batch_job, 'dest') and hasattr(batch_job.dest, 'file_name'):
                result_file_name = batch_job.dest.file_name

                # Download result file content
                result_content = self.client.files.download(file=result_file_name)

                # Save results to local file
                if batch_file_path:
                    results_file_path = batch_file_path.replace('_requests.jsonl', '_results.jsonl')
                    with open(results_file_path, 'wb') as f:
                        f.write(result_content)
                    self.logger.info(f"Saved batch results to: {results_file_path}")

                # Parse JSONL results
                for line in result_content.decode('utf-8').strip().split('\n'):
                    if line:
                        results.append(json.loads(line))

            elif hasattr(batch_job, 'dest') and hasattr(batch_job.dest, 'inlined_responses'):
                # Fallback to inline responses if available
                results = batch_job.dest.inlined_responses

                # Save inline results to file
                if batch_file_path:
                    results_file_path = batch_file_path.replace('_requests.jsonl', '_results.jsonl')
                    with open(results_file_path, 'w', encoding='utf-8') as f:
                        for item in results:
                            f.write(json.dumps(item) + '\n')
                    self.logger.info(f"Saved batch results to: {results_file_path}")
            else:
                self.logger.warning("Could not find results in batch job response")

        except Exception as e:
            self.logger.error(f"Error downloading results: {e}")

        self.logger.info(f"Downloaded {len(results)} results")
        return results

    def _parse_batch_results(self, results: List[Dict[str, Any]],
                              num_prompts: int) -> List[List[str]]:
        """Parse batch results into response format"""
        response_map = {}

        for item in results:
            try:
                # Extract key and response
                key = item.get('key', '')
                index = int(key.split('-')[1])

                # Extract response text - handle multiple possible formats
                response = item.get('response', {})

                if hasattr(response, 'text'):
                    # Direct text attribute
                    response_map[index] = response.text
                elif isinstance(response, dict):
                    # Dictionary format
                    candidates = response.get('candidates', [])
                    if candidates:
                        content = candidates[0].get('content', {})
                        parts = content.get('parts', [])
                        if parts:
                            response_map[index] = parts[0].get('text', '')
                    else:
                        # Try direct text field
                        response_map[index] = response.get('text', '')
                elif isinstance(response, str):
                    response_map[index] = response
                else:
                    response_map[index] = str(response)

            except (ValueError, IndexError, KeyError) as e:
                self.logger.warning(f"Error parsing result: {e}")

        # Build ordered response list
        responses = []
        for i in range(num_prompts):
            if i in response_map:
                responses.append([response_map[i]])
            else:
                self.logger.warning(f"Missing response for index {i}")
                responses.append([""])

        return responses

    def run_inference(self, prompts: List[str], temperature: float = 0.0,
                      max_tokens: int = 4096, dataset_name: str = None,
                      **kwargs) -> List[List[str]]:
        """Run batch inference using Gemini Batch API with file-based requests"""
        if not prompts:
            return []

        # Step 1: Create batch request file
        batch_file_path = self._create_batch_file(prompts, temperature, max_tokens, dataset_name)

        # Step 2: Ask user to confirm before submitting
        print(f"\n{'='*60}")
        print(f"Batch request file created: {batch_file_path}")
        print(f"Number of requests: {len(prompts)}")
        print(f"Model: {self.model_name}")
        temp_str = str(temperature) if temperature is not None else "API default"
        max_tokens_str = str(max_tokens) if max_tokens is not None else "API default"
        print(f"Temperature: {temp_str}, Max tokens: {max_tokens_str}")
        print(f"{'='*60}")

        # Show preview of first 3 requests
        print(f"\n--- Preview of first {min(3, len(prompts))} request(s) ---")
        with open(batch_file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= 3:
                    break
                req = json.loads(line)
                # Extract the prompt text from the Gemini request format
                contents = req.get('request', {}).get('contents', [])
                if contents and len(contents) > 0:
                    parts = contents[0].get('parts', [])
                    if parts and len(parts) > 0:
                        prompt_text = parts[0].get('text', '')
                        print(f"\n[Request {i+1}]")
                        print(prompt_text)
        print(f"\n{'='*60}")

        # Skip confirmation if auto_submit is enabled
        if self.auto_submit:
            self.logger.info("Auto-submit enabled, proceeding with batch submission...")
            print("Auto-submit enabled, proceeding with batch submission...")
        else:
            confirm = input("Do you want to proceed with submitting this batch? (yes/no): ").strip().lower()
            if confirm not in ['yes', 'y']:
                self.logger.info("Batch submission cancelled by user.")
                print("Batch submission cancelled. You can review the file at:")
                print(f"  {batch_file_path}")
                return []

        # Step 3: Upload file to Gemini
        uploaded_file = self._upload_batch_file(batch_file_path)

        # Step 3: Create batch job
        batch_job = self._create_batch_job(uploaded_file)

        # Step 4: Poll for completion
        job_result = self._poll_batch_status(batch_job.name)

        # Step 5: Download and parse results
        results = self._download_results(job_result['batch_job'], batch_file_path)

        # Step 6: Parse into response format
        responses = self._parse_batch_results(results, len(prompts))

        return responses

    def run_inference_pipeline(self, data_path: str, output_dir: str,
                                temperature: float = 0.0, max_tokens: int = 4096,
                                max_samples: int = None, seed: int = 42,
                                balanced_sampling: bool = True,
                                prompt_column: str = None,
                                num_samples: int = 1, **kwargs) -> InferenceResult:
        """Run complete inference pipeline using Gemini Batch API"""
        # Extract dataset name and create output directory
        dataset_name = os.path.splitext(os.path.basename(data_path))[0]
        dataset_output_dir = os.path.join(output_dir, dataset_name)
        os.makedirs(dataset_output_dir, exist_ok=True)

        # Load data
        self.logger.info(f"Loading data from {data_path}")
        data = self.load_data(data_path)

        if max_samples:
            if balanced_sampling:
                data = self.balanced_sample(data, max_samples, seed)
                self.logger.info(f"Using balanced sample of {len(data)} samples for inference")
            else:
                data = data[:max_samples]
                self.logger.info(f"Using first {len(data)} samples for inference")

        # Prepare prompts
        prompts = self.prepare_prompts_from_data(data, prompt_column)
        self.logger.info(f"Prepared {len(prompts)} prompts")

        # Run batch inference
        self.logger.info("Starting Gemini Batch API inference...")
        responses = self.run_inference(prompts, temperature, max_tokens, dataset_name=dataset_name)

        # Create result
        result = InferenceResult(
            prompts=prompts,
            responses=responses,
            metadata=data
        )

        # Save results
        model_name = self.output_model_name or self.model_name
        inference_file = os.path.join(
            dataset_output_dir,
            f"{model_name}_{self.template}_{num_samples}_inference_results.jsonl"
        )

        inference_params = {
            'temperature': temperature,
            'max_tokens': max_tokens,
            'seed': seed,
            'num_samples': num_samples,
            'backend': 'gemini',
            'model': self.model_name
        }

        self.save_inference_results(prompts, responses, data, inference_file, inference_params)
        self.logger.info(f"Inference completed. Results saved to {inference_file}")

        return result
