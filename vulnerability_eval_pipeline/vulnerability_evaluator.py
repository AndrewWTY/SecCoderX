#!/usr/bin/env python3
"""
Vulnerability Detection Evaluator
Processes inference results and computes evaluation metrics
"""

import json
import re
import argparse
import logging
import os
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
from collections import defaultdict
import numpy as np
from tqdm import tqdm

# Metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

@dataclass
class EvaluationResult:
    """Store evaluation results and metrics"""
    predictions: List[bool]
    ground_truth: List[bool]
    model_responses: List[List[str]]  # Now supports multiple responses per prompt
    accuracy: float
    precision: float
    recall: float
    f1: float
    sensitivity: float
    balanced_accuracy: float
    confusion_matrix: np.ndarray
    classification_report: str
    per_class_metrics: Dict[str, Dict[str, float]]  # Per-class metrics for vulnerable/non-vulnerable
    none_predictions_total: int = 0
    none_predictions_vulnerable: int = 0
    none_predictions_not_vulnerable: int = 0

class VulnerabilityEvaluator:
    def __init__(self, label_column: str = "is_vulnerable", is_think_response: bool = False):
        """Initialize the vulnerability evaluator"""
        # Setup logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        self.label_column = label_column
        self.is_think_response = is_think_response
    
    def load_inference_results(self, inference_file: str) -> Tuple[List[str], List[List[str]], List[Dict[str, Any]]]:
        """Load inference results from JSONL file, supporting both single and multi-sampling"""
        prompts = []
        responses = []
        metadata = []
        
        with open(inference_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    item = json.loads(line)
                    prompts.append(item['prompt'])
                    
                    # Handle both old format (single response) and new format (multiple responses)
                    if 'responses' in item:
                        # New format with multiple responses
                        responses.append(item['responses'])
                    elif 'response' in item:
                        # Old format with single response - wrap in list for consistency
                        responses.append([item['response']])
                    else:
                        raise ValueError(f"No 'response' or 'responses' field found in inference file")
                    
                    metadata.append(item['metadata'])
        
        return prompts, responses, metadata
    
    def extract_prediction(self, response: str) -> bool:
        """Extract YES/NO prediction from model response, returns None if unclear"""
        response = response.strip().upper()
        # import pdb; pdb.set_trace()
        if self.is_think_response:
            # First, try to extract content between <output> tags and look for YES/NO
            output_pattern = r'<output>(.*?)</output>'
            output_match = re.search(output_pattern, response, re.IGNORECASE | re.DOTALL)
            if output_match:
                output_content = output_match.group(1).strip().upper()
                # Look for YES or NO in the output
                if 'NO' in output_content:
                    return False
                elif 'YES' in output_content:
                    return True
                else:
                    # If output tag exists but no clear YES/NO, continue to answer tag fallback
                    pass
            
            # Fallback: Extract content between <answer> tags
            answer_pattern = r'<answer>(.*?)</answer>'
            match = re.search(answer_pattern, response, re.IGNORECASE | re.DOTALL)
            # import pdb; pdb.set_trace()
            if match:
                answer_content = match.group(1).strip().upper()
                # import pdb; pdb.set_trace()
                # Look for "vulnerable" or "not vulnerable" in the answer
                if 'NOT VULNERABLE' in answer_content or 'NOT_VULNERABLE' in answer_content:
                    return False
                elif 'VULNERABLE' in answer_content:
                    return True
                else:
                    # self.logger.warning(f"Could not extract clear prediction from answer tag: {answer_content}")
                    return None
            else:
                # self.logger.warning(f"No <answer> tag found in response.")
                return None
                
        else:
            # Original logic for [[YES]]/[[NO]] patterns
            yes_pattern = r'\[\[YES\]\]'
            no_pattern = r'\[\[NO\]\]'
            
            if re.search(yes_pattern, response):
                return True
            elif re.search(no_pattern, response):
                return False
            else:
                # Fallback: look for YES or NO in the response
                if 'YES' in response:
                    return True
                elif 'NO' in response:
                    return False
                else:
                    # self.logger.warning(f"Could not extract clear prediction from response.")
                    return None
    
    def majority_vote(self, response_list: List[str]) -> bool:
        """Apply majority voting to multiple responses for a single prompt"""
        predictions = []
        for response in response_list:
            pred = self.extract_prediction(response)
            if pred is not None:  # Only count clear predictions
                predictions.append(pred)
        
        if not predictions:
            # No clear predictions from any response
            return None
        
        # Count votes
        true_votes = sum(predictions)
        false_votes = len(predictions) - true_votes
        # import pdb; pdb.set_trace()
        
        # Return majority vote
        if true_votes > false_votes:
            return True
        elif false_votes > true_votes:
            return False
        else:
            # Tie - return None for unclear
            return False
    
    def evaluate_from_inference_results(self, inference_file: str) -> EvaluationResult:
        """Evaluate predictions from inference results file"""
        
        self.logger.info(f"Loading inference results from {inference_file}")
        prompts, responses, metadata = self.load_inference_results(inference_file)
        
        # Extract predictions using majority voting for multi-sampling
        raw_predictions = []  # Keep original predictions including None
        predictions = []      # Convert None to opposite of ground truth (always wrong)
        for response_list in responses:
            if len(response_list) == 1:
                # Single response - use direct prediction
                pred = self.extract_prediction(response_list[0])
            else:
                # Multiple responses - use majority voting
                pred = self.majority_vote(response_list)
                # import pdb; pdb.set_trace()
            raw_predictions.append(pred)
        
        # Get ground truth from metadata
        ground_truth = []
        for meta in metadata:
            # import pdb; pdb.set_trace()
            gt = meta.get(self.label_column, False)
            ground_truth.append(gt)
        
        # Convert None predictions to opposite of ground truth (making them always wrong)
        none_count = 0
        none_counts_by_class = {'vulnerable': 0, 'not_vulnerable': 0}
        
        for i, pred in enumerate(raw_predictions):
            if pred is None:
                none_count += 1
                # Make None prediction always wrong by setting to opposite of ground truth
                predictions.append(not ground_truth[i])
                
                # Track None counts by true class
                if ground_truth[i]:
                    none_counts_by_class['vulnerable'] += 1
                else:
                    none_counts_by_class['not_vulnerable'] += 1
            else:
                predictions.append(pred)
        
        # Calculate metrics using all predictions (None converted to wrong predictions)
        # We will use sklearn.metrics to ensure correctness
        accuracy = accuracy_score(ground_truth, predictions)
        precision = precision_score(ground_truth, predictions, zero_division=0)
        recall = recall_score(ground_truth, predictions, zero_division=0)
        f1 = f1_score(ground_truth, predictions, zero_division=0)
        
        # Additional metrics
        sensitivity = recall  # Sensitivity is the same as recall
        # Balanced accuracy = (sensitivity + specificity) / 2
        # Specificity = TN / (TN + FP)
        from sklearn.metrics import balanced_accuracy_score
        balanced_accuracy = balanced_accuracy_score(ground_truth, predictions)
        
        # Create 2x3 confusion matrix (ground truth x [vuln, not_vuln, none]) FIRST
        extended_cm = np.zeros((2, 3), dtype=int)
        for gt, pred in zip(ground_truth, raw_predictions):
            gt_idx = 0 if gt else 1  # 0 = Vulnerable, 1 = Not Vulnerable
            if pred is None:
                pred_idx = 2  # None column
            else:
                pred_idx = 0 if pred else 1  # 0 = Vulnerable, 1 = Not Vulnerable
            extended_cm[gt_idx, pred_idx] += 1
        
        # Per-class metrics calculation
        from sklearn.metrics import precision_recall_fscore_support
        per_class_precision, per_class_recall, per_class_f1, per_class_support = precision_recall_fscore_support(
            ground_truth, predictions, labels=[False, True], zero_division=0
        )
        
        # Calculate per-class balanced accuracy using the standard 2x2 confusion matrix
        # We need to create a standard confusion matrix from sklearn
        standard_cm = confusion_matrix(ground_truth, predictions, labels=[False, True])
        
        # Extract values from standard confusion matrix
        # standard_cm[i,j] where i=true label, j=predicted label
        # For labels=[False, True]: 
        # [0,0] = TN (True Negative: Not Vulnerable predicted as Not Vulnerable)
        # [0,1] = FP (False Positive: Not Vulnerable predicted as Vulnerable) 
        # [1,0] = FN (False Negative: Vulnerable predicted as Not Vulnerable)
        # [1,1] = TP (True Positive: Vulnerable predicted as Vulnerable)
        
        tn_std = standard_cm[0,0]  # Not Vulnerable → Not Vulnerable (TN)
        fp_std = standard_cm[0,1]  # Not Vulnerable → Vulnerable (FP)
        fn_std = standard_cm[1,0]  # Vulnerable → Not Vulnerable (FN)
        tp_std = standard_cm[1,1]  # Vulnerable → Vulnerable (TP)
        
        # Per-class balanced accuracy calculation
        # For class 0 (Not Vulnerable): BA = (specificity + sensitivity) / 2
        # specificity_class0 = TN/(TN+FP), sensitivity_class0 = TN/(TN+FN)
        if (tn_std + fp_std) > 0:
            specificity_class0 = tn_std / (tn_std + fp_std)  # True Negative Rate for class 0
        else:
            specificity_class0 = 0.0
            
        if (tn_std + fn_std) > 0:
            sensitivity_class0 = tn_std / (tn_std + fn_std)  # This is actually precision for negative class
        else:
            sensitivity_class0 = 0.0
            
        balanced_accuracy_class0 = (specificity_class0 + sensitivity_class0) / 2.0
        
        # For class 1 (Vulnerable): BA = (sensitivity + specificity) / 2  
        # sensitivity_class1 = TP/(TP+FN), specificity_class1 = TN/(TN+FP)
        if (tp_std + fn_std) > 0:
            sensitivity_class1 = tp_std / (tp_std + fn_std)  # True Positive Rate (Recall)
        else:
            sensitivity_class1 = 0.0
            
        if (tn_std + fp_std) > 0:
            specificity_class1 = tn_std / (tn_std + fp_std)  # True Negative Rate
        else:
            specificity_class1 = 0.0
            
        balanced_accuracy_class1 = (sensitivity_class1 + specificity_class1) / 2.0
        
        # Store per-class metrics
        per_class_metrics = {
            'not_vulnerable': {
                'precision': float(per_class_precision[0]),
                'recall': float(per_class_recall[0]),
                'f1': float(per_class_f1[0]),
                'sensitivity': float(sensitivity_class0),
                'balanced_accuracy': float(balanced_accuracy_class0),
                'support': int(per_class_support[0])
            },
            'vulnerable': {
                'precision': float(per_class_precision[1]),
                'recall': float(per_class_recall[1]),
                'f1': float(per_class_f1[1]),
                'sensitivity': float(sensitivity_class1),
                'balanced_accuracy': float(balanced_accuracy_class1),
                'support': int(per_class_support[1])
            }
        }

        # Standard classification report
        report = classification_report(ground_truth, predictions, 
                                     target_names=['Not Vulnerable', 'Vulnerable'],
                                     zero_division=0)
        
        # # Print extended confusion matrix
        # print("\nExtended Confusion Matrix:")
        # print("Ground Truth \\ Prediction | Vulnerable | Not Vuln | Unclear")
        # print("-" * 55)
        # print(f"Vulnerable              | {extended_cm[0,0]:^8} | {extended_cm[0,1]:^9} | {extended_cm[0,2]:^7}")
        # print(f"Not Vulnerable          | {extended_cm[1,0]:^8} | {extended_cm[1,1]:^9} | {extended_cm[1,2]:^7}")
        #
        # # Extract and print detailed metrics calculations
        # tp_manual = tp_std
        # tn_manual = tn_std
        # fp_manual = fp_std
        # fn_manual = fn_std
        #
        # print(f"\nDetailed Metrics Calculation:")
        # print(f"TP (Vulnerable → Vulnerable): {tp_manual}")
        # print(f"TN (Not Vulnerable → Not Vulnerable): {tn_manual}")
        # print(f"FP (Not Vulnerable → Vulnerable): {fp_manual}")
        # print(f"FN (Vulnerable → Not Vulnerable): {fn_manual}")
        #
        # accuracy_manual = (tp_manual + tn_manual) / (tp_manual + tn_manual + fp_manual + fn_manual)
        # precision_manual = tp_manual / (tp_manual + fp_manual) if (tp_manual + fp_manual) > 0 else 0.0
        # recall_manual = tp_manual / (tp_manual + fn_manual) if (tp_manual + fn_manual) > 0 else 0.0
        # f1_manual = 2 * (precision_manual * recall_manual) / (precision_manual + recall_manual) if (precision_manual + recall_manual) > 0 else 0.0
        # sensitivity_manual = recall_manual
        # specificity_manual = tn_manual / (tn_manual + fp_manual) if (tn_manual + fp_manual) > 0 else 0.0
        # balanced_accuracy_manual = (sensitivity_manual + specificity_manual) / 2.0
        #
        # print(f"\nManual Calculations:")
        # print(f"Accuracy = (TP + TN) / Total = ({tp_manual} + {tn_manual}) / {tp_manual + tn_manual + fp_manual + fn_manual} = {accuracy_manual:.4f}")
        # print(f"Precision = TP / (TP + FP) = {tp_manual} / ({tp_manual} + {fp_manual}) = {precision_manual:.4f}")
        # print(f"Recall/Sensitivity = TP / (TP + FN) = {tp_manual} / ({tp_manual} + {fn_manual}) = {recall_manual:.4f}")
        # print(f"Specificity = TN / (TN + FP) = {tn_manual} / ({tn_manual} + {fp_manual}) = {specificity_manual:.4f}")
        # print(f"F1 = 2 * (Precision * Recall) / (Precision + Recall) = 2 * ({precision_manual:.4f} * {recall_manual:.4f}) / ({precision_manual:.4f} + {recall_manual:.4f}) = {f1_manual:.4f}")
        # print(f"Balanced Accuracy = (Sensitivity + Specificity) / 2 = ({sensitivity_manual:.4f} + {specificity_manual:.4f}) / 2 = {balanced_accuracy_manual:.4f}")
        #
        # print(f"\nSklearn Calculations:")
        # print(f"Accuracy: {accuracy:.4f}")
        # print(f"Precision: {precision:.4f}")
        # print(f"Recall/Sensitivity: {recall:.4f}")
        # print(f"F1: {f1:.4f}")
        # print(f"Balanced Accuracy: {balanced_accuracy:.4f}")
        #
        # print(f"\nPer-Class Metrics:")
        # print(f"Not Vulnerable Class:")
        # print(f"  Precision: {per_class_metrics['not_vulnerable']['precision']:.4f}")
        # print(f"  Recall: {per_class_metrics['not_vulnerable']['recall']:.4f}")
        # print(f"  F1: {per_class_metrics['not_vulnerable']['f1']:.4f}")
        # print(f"  Sensitivity: {per_class_metrics['not_vulnerable']['sensitivity']:.4f}")
        # print(f"  Balanced Accuracy: {per_class_metrics['not_vulnerable']['balanced_accuracy']:.4f}")
        # print(f"  Support: {per_class_metrics['not_vulnerable']['support']}")
        #
        # print(f"Vulnerable Class:")
        # print(f"  Precision: {per_class_metrics['vulnerable']['precision']:.4f}")
        # print(f"  Recall: {per_class_metrics['vulnerable']['recall']:.4f}")
        # print(f"  F1: {per_class_metrics['vulnerable']['f1']:.4f}")
        # print(f"  Sensitivity: {per_class_metrics['vulnerable']['sensitivity']:.4f}")
        # print(f"  Balanced Accuracy: {per_class_metrics['vulnerable']['balanced_accuracy']:.4f}")
        # print(f"  Support: {per_class_metrics['vulnerable']['support']}")
        #
        # print(f"\nNote: {none_count} unclear predictions treated as wrong (opposite of ground truth)")
        #
        # self.logger.info(f"Evaluation completed. Accuracy: {accuracy:.4f}, F1: {f1:.4f}")
        # self.logger.info(f"Total unclear predictions (None): {none_count} - all treated as incorrect")
        # self.logger.info(f"None predictions for vulnerable cases: {none_counts_by_class['vulnerable']}")
        # self.logger.info(f"None predictions for non-vulnerable cases: {none_counts_by_class['not_vulnerable']}")
        
        return EvaluationResult(
            predictions=predictions,  # Includes None converted to wrong predictions
            ground_truth=ground_truth,
            model_responses=responses,
            accuracy=accuracy,
            precision=precision,
            recall=recall,
            f1=f1,
            sensitivity=sensitivity,
            balanced_accuracy=balanced_accuracy,
            confusion_matrix=extended_cm,
            classification_report=report,
            per_class_metrics=per_class_metrics,
            none_predictions_total=none_count,
            none_predictions_vulnerable=none_counts_by_class['vulnerable'],
            none_predictions_not_vulnerable=none_counts_by_class['not_vulnerable']
        )
    
    def analyze_by_language(self, metadata: List[Dict[str, Any]], result: EvaluationResult) -> Dict[str, Dict[str, float]]:
        """Analyze results by programming language"""
        language_data = defaultdict(lambda: {'predictions': [], 'ground_truth': []})
        
        # Get the original predictions including None using majority voting
        all_predictions = []
        for response_list in result.model_responses:
            if len(response_list) == 1:
                pred = self.extract_prediction(response_list[0])
            else:
                pred = self.majority_vote(response_list)
            all_predictions.append(pred)
        
        for i, meta in enumerate(metadata):
            lang = meta.get('language_name', 'Unknown')
            pred = all_predictions[i]
            gt = meta.get(self.label_column, False)
            
            # Convert None to opposite of ground truth (always wrong)
            if pred is None:
                pred = not gt
            
            language_data[lang]['predictions'].append(pred)
            language_data[lang]['ground_truth'].append(gt)
        
        language_metrics = {}
        for lang, data in language_data.items():
            if len(data['ground_truth']) > 0:
                preds = data['predictions']
                truth = data['ground_truth']
                
                # Calculate from scratch
                accuracy = accuracy_score(truth, preds)
                precision = precision_score(truth, preds, zero_division=0)
                recall = recall_score(truth, preds, zero_division=0)
                f1 = f1_score(truth, preds, zero_division=0)
                
                # Additional metrics
                sensitivity = recall
                from sklearn.metrics import balanced_accuracy_score
                balanced_accuracy = balanced_accuracy_score(truth, preds)
                
                # Per-class metrics for this language
                from sklearn.metrics import precision_recall_fscore_support
                lang_per_class_precision, lang_per_class_recall, lang_per_class_f1, lang_per_class_support = precision_recall_fscore_support(
                    truth, preds, labels=[False, True], zero_division=0
                )
                
                language_metrics[lang] = {
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1': f1,
                    'sensitivity': sensitivity,
                    'balanced_accuracy': balanced_accuracy,
                    'count': len(truth),
                    'per_class_metrics': {
                        'not_vulnerable': {
                            'precision': float(lang_per_class_precision[0]),
                            'recall': float(lang_per_class_recall[0]),
                            'f1': float(lang_per_class_f1[0]),
                            'support': int(lang_per_class_support[0])
                        },
                        'vulnerable': {
                            'precision': float(lang_per_class_precision[1]),
                            'recall': float(lang_per_class_recall[1]),
                            'f1': float(lang_per_class_f1[1]),
                            'support': int(lang_per_class_support[1])
                        }
                    }
                }
        
        return language_metrics
    
    def analyze_by_cwe(self, metadata: List[Dict[str, Any]], result: EvaluationResult) -> Dict[str, Dict[str, float]]:
        """Analyze results by CWE type"""
        cwe_data = defaultdict(lambda: {'predictions': [], 'ground_truth': []})
        
        # Get the original predictions including None using majority voting
        all_predictions = []
        for response_list in result.model_responses:
            if len(response_list) == 1:
                pred = self.extract_prediction(response_list[0])
            else:
                pred = self.majority_vote(response_list)
            all_predictions.append(pred)
        
        for i, meta in enumerate(metadata):
            cwe = meta.get('cwe', 'Unknown')
            pred = all_predictions[i]
            gt = meta.get(self.label_column, False)
            
            # Convert None to opposite of ground truth (always wrong)
            if pred is None:
                pred = not gt
            
            # Handle case where cwe is now a list
            if isinstance(cwe, list):
                if len(cwe) > 0:
                    # Add entry for each CWE in the list
                    for individual_cwe in cwe:
                        cwe_data[individual_cwe]['predictions'].append(pred)
                        cwe_data[individual_cwe]['ground_truth'].append(gt)
                else:
                    # Empty list - treat as unknown
                    cwe_data['Unknown']['predictions'].append(pred)
                    cwe_data['Unknown']['ground_truth'].append(gt)
            elif cwe:
                # Single CWE (original format)
                cwe_data[cwe]['predictions'].append(pred)
                cwe_data[cwe]['ground_truth'].append(gt)
            else:
                # No CWE or empty string
                cwe_data['Unknown']['predictions'].append(pred)
                cwe_data['Unknown']['ground_truth'].append(gt)
        
        cwe_metrics = {}
        for cwe, data in cwe_data.items():
            if len(data['ground_truth']) > 0:
                preds = data['predictions']
                truth = data['ground_truth']
                
                # Calculate from scratch
                accuracy = accuracy_score(truth, preds)
                precision = precision_score(truth, preds, zero_division=0)
                recall = recall_score(truth, preds, zero_division=0)
                f1 = f1_score(truth, preds, zero_division=0)
                
                # Additional metrics
                sensitivity = recall
                from sklearn.metrics import balanced_accuracy_score
                balanced_accuracy = balanced_accuracy_score(truth, preds)
                
                # Per-class metrics for this CWE
                from sklearn.metrics import precision_recall_fscore_support
                cwe_per_class_precision, cwe_per_class_recall, cwe_per_class_f1, cwe_per_class_support = precision_recall_fscore_support(
                    truth, preds, labels=[False, True], zero_division=0
                )
                
                cwe_metrics[cwe] = {
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1': f1,
                    'sensitivity': sensitivity,
                    'balanced_accuracy': balanced_accuracy,
                    'count': len(truth),
                    'per_class_metrics': {
                        'not_vulnerable': {
                            'precision': float(cwe_per_class_precision[0]),
                            'recall': float(cwe_per_class_recall[0]),
                            'f1': float(cwe_per_class_f1[0]),
                            'support': int(cwe_per_class_support[0])
                        },
                        'vulnerable': {
                            'precision': float(cwe_per_class_precision[1]),
                            'recall': float(cwe_per_class_recall[1]),
                            'f1': float(cwe_per_class_f1[1]),
                            'support': int(cwe_per_class_support[1])
                        }
                    }
                }
        
        return cwe_metrics
    
    def save_evaluation_results(self, result: EvaluationResult, metadata: List[Dict[str, Any]], 
                               output_file: str, model_name: str = "model"):
        """Save comprehensive evaluation results"""
        
        # Analyze by language and CWE
        language_metrics = self.analyze_by_language(metadata, result)
        cwe_metrics = self.analyze_by_cwe(metadata, result)
        
        # Prepare comprehensive results
        eval_results = {
            "model_name": model_name,
            "configuration": {
                "label_column": self.label_column,
                "is_think_response": self.is_think_response
            },
            "overall_metrics": {
                "accuracy": float(result.accuracy),
                "precision": float(result.precision),
                "recall": float(result.recall),
                "f1": float(result.f1),
                "sensitivity": float(result.sensitivity),
                "balanced_accuracy": float(result.balanced_accuracy),
                "total_samples": len(result.predictions),
                "unclear_predictions": {
                    "total": result.none_predictions_total,
                    "vulnerable_cases": result.none_predictions_vulnerable,
                    "not_vulnerable_cases": result.none_predictions_not_vulnerable
                }
            },
            "per_class_metrics": {
                "not_vulnerable": {
                    "precision": result.per_class_metrics["not_vulnerable"]["precision"],
                    "recall": result.per_class_metrics["not_vulnerable"]["recall"],
                    "f1": result.per_class_metrics["not_vulnerable"]["f1"],
                    "sensitivity": result.per_class_metrics["not_vulnerable"]["sensitivity"],
                    "balanced_accuracy": result.per_class_metrics["not_vulnerable"]["balanced_accuracy"],
                    "support": result.per_class_metrics["not_vulnerable"]["support"]
                },
                "vulnerable": {
                    "precision": result.per_class_metrics["vulnerable"]["precision"],
                    "recall": result.per_class_metrics["vulnerable"]["recall"],
                    "f1": result.per_class_metrics["vulnerable"]["f1"],
                    "sensitivity": result.per_class_metrics["vulnerable"]["sensitivity"],
                    "balanced_accuracy": result.per_class_metrics["vulnerable"]["balanced_accuracy"],
                    "support": result.per_class_metrics["vulnerable"]["support"]
                }
            },
            "extended_confusion_matrix": {
                "matrix": result.confusion_matrix.tolist(),
                "labels": {
                    "rows": ["Vulnerable", "Not Vulnerable"],
                    "columns": ["Vulnerable", "Not Vulnerable", "Unclear"]
                },
                "format": "2x3 matrix where rows are ground truth and columns are predictions"
            },
            "classification_report": result.classification_report,
            "metrics_by_language": {
                lang: {
                    "accuracy": float(metrics["accuracy"]),
                    "precision": float(metrics["precision"]),
                    "recall": float(metrics["recall"]),
                    "f1": float(metrics["f1"]),
                    "sensitivity": float(metrics["sensitivity"]),
                    "balanced_accuracy": float(metrics["balanced_accuracy"]),
                    "count": metrics["count"],
                    "per_class_metrics": metrics["per_class_metrics"]
                }
                for lang, metrics in language_metrics.items()
            },
            "metrics_by_cwe": {
                cwe: {
                    "accuracy": float(metrics["accuracy"]),
                    "precision": float(metrics["precision"]),
                    "recall": float(metrics["recall"]),
                    "f1": float(metrics["f1"]),
                    "sensitivity": float(metrics["sensitivity"]),
                    "balanced_accuracy": float(metrics["balanced_accuracy"]),
                    "count": metrics["count"],
                    "per_class_metrics": metrics["per_class_metrics"]
                }
                for cwe, metrics in cwe_metrics.items()
            }
        }
        
        # Save results
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(eval_results, f, indent=2)
        
        self.logger.info(f"Evaluation results saved to {output_file}")
    
    def save_detailed_predictions(self, result: EvaluationResult, metadata: List[Dict[str, Any]], 
                                 output_file: str):
        """Save detailed predictions with all information"""
        
        # Get the original predictions including None using majority voting
        all_predictions = []
        individual_predictions = []  # Store all individual predictions per prompt
        for response_list in result.model_responses:
            if len(response_list) == 1:
                pred = self.extract_prediction(response_list[0])
                individual_preds = [self.extract_prediction(response_list[0])]
            else:
                pred = self.majority_vote(response_list)
                individual_preds = [self.extract_prediction(resp) for resp in response_list]
            all_predictions.append(pred)
            individual_predictions.append(individual_preds)
        
        # Get ground truth for all samples
        all_ground_truth = []
        for meta in metadata:
            gt = meta.get(self.label_column, False)
            all_ground_truth.append(gt)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for i, (pred, truth, response_list, meta, individual_preds) in enumerate(zip(
                all_predictions, all_ground_truth, result.model_responses, metadata, individual_predictions
            )):
                detailed_item = {
                    'index': i,
                    'prediction': pred,  # Final prediction after majority voting (can be True, False, or None)
                    'individual_predictions': individual_preds,  # All individual predictions
                    'ground_truth': truth,
                    'correct': pred == truth if pred is not None else False,
                    'unclear_prediction': pred is None,
                    'model_responses': response_list,  # All responses for this prompt
                    'num_samples': len(response_list),
                    'metadata': meta
                }
                f.write(json.dumps(detailed_item) + '\n')
        
        self.logger.info(f"Detailed predictions saved to {output_file}")
    
    def run_evaluation_pipeline(self, inference_file: str, output_dir: str, 
                               label_column: str = None, is_think_response: bool = None) -> EvaluationResult:
        """Run complete evaluation pipeline from inference results"""
        
        # Update instance variables if provided
        if label_column is not None:
            self.label_column = label_column
        if is_think_response is not None:
            self.is_think_response = is_think_response
        
        # Extract dataset name from inference file path
        # The inference file is typically in format: {output_dir}/{dataset_name}/{model_name}_inference_results.jsonl
        inference_dir = os.path.dirname(inference_file)
        dataset_name = os.path.basename(inference_dir)
        
        # Create dataset-specific output directory
        dataset_output_dir = os.path.join(output_dir, dataset_name)
        os.makedirs(dataset_output_dir, exist_ok=True)
        
        # Load inference results and evaluate
        result = self.evaluate_from_inference_results(inference_file)
        
        # Load metadata for analysis
        _, _, metadata = self.load_inference_results(inference_file)
        
        # Generate output filenames
        base_name = os.path.splitext(os.path.basename(inference_file))[0]
        model_name = base_name.replace('_inference_results', '')
        
        eval_results_file = os.path.join(dataset_output_dir, f"{model_name}_evaluation_results.json")
        detailed_results_file = os.path.join(dataset_output_dir, f"{model_name}_detailed_predictions.jsonl")
        
        # Save results
        self.save_evaluation_results(result, metadata, eval_results_file, model_name)
        # self.save_detailed_predictions(result, metadata, detailed_results_file)
        
        # Print summary
        print(f"\n=== Evaluation Summary ===")
        print(f"Model: {model_name}")
        print(f"Label column: {self.label_column}")
        print(f"Think response mode: {self.is_think_response}")
        print(f"Total samples: {len(metadata)}")
        print(f"Accuracy: {result.accuracy:.4f}")
        print(f"Precision: {result.precision:.4f}")
        print(f"Recall: {result.recall:.4f}")
        print(f"F1 Score: {result.f1:.4f}")
        print(f"Sensitivity: {result.sensitivity:.4f}")
        print(f"Balanced Accuracy: {result.balanced_accuracy:.4f}")
        print(f"Unclear predictions: {result.none_predictions_total} ({result.none_predictions_total/len(metadata)*100:.1f}%) - all treated as incorrect")
        print(f"  - For vulnerable cases: {result.none_predictions_vulnerable} (treated as not vulnerable)")
        print(f"  - For non-vulnerable cases: {result.none_predictions_not_vulnerable} (treated as vulnerable)")
        print(f"\nResults saved to:")
        print(f"  - {eval_results_file}")
        print(f"  - {detailed_results_file}")
        
        return result

def main():
    parser = argparse.ArgumentParser(description="Evaluate vulnerability detection results")
    parser.add_argument("--inference_file", type=str, required=True, 
                       help="Path to inference results JSONL file")
    parser.add_argument("--output_dir", type=str, default="./eval_results", 
                       help="Output directory for evaluation results")
    parser.add_argument("--label_column", type=str, default="is_vulnerable",
                       help="Column name for ground truth labels in metadata (default: is_vulnerable)")
    parser.add_argument("--is_think_response", action="store_true",
                       help="Enable think response mode - extract answers from <answer> tags and match 'vulnerable'/'not vulnerable'")
    
    args = parser.parse_args()
    
    # Initialize evaluator with the specified parameters
    evaluator = VulnerabilityEvaluator(
        label_column=args.label_column,
        is_think_response=args.is_think_response
    )
    
    # Run evaluation
    result = evaluator.run_evaluation_pipeline(
        inference_file=args.inference_file,
        output_dir=args.output_dir
    )

if __name__ == "__main__":
    main()