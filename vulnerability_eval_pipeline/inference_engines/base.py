"""
Base Inference Engine - Abstract base class for all inference backends
"""

import json
import logging
import os
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
import numpy as np


@dataclass
class InferenceResult:
    """Store inference results"""
    prompts: List[str]
    responses: List[List[str]]  # List of responses per prompt (supports multi-sampling)
    metadata: List[Dict[str, Any]]


class BaseInferenceEngine(ABC):
    """Abstract base class for inference engines"""

    # Template mapping - shared across all engines
    TEMPLATE_MAP = {
        "r2vul": "prompt_templates/r2vul_user.txt",
        "think_with_cwe_new": "prompt_templates/think_templates_withcwe_new.txt",
        "think_with_cwe_new_blackbox": "prompt_templates/think_templates_withcwe_new_blackbox.txt"
    }

    def __init__(self, template: str = None, system_prompt_file: str = None,
                 output_model_name: str = None, max_prompt_tokens: int = None):
        """
        Initialize base inference engine.

        Args:
            template: Template type or path to template file
            system_prompt_file: Path to system prompt file
            output_model_name: Name to use in output files
            max_prompt_tokens: Maximum tokens for prompts (for truncation)
        """
        # Setup logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(self.__class__.__name__)

        # Get project root directory (must be set before loading templates)
        self.project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

        # Store configuration
        self.template = template
        self.output_model_name = output_model_name
        self.max_prompt_tokens = max_prompt_tokens or 30000

        # Load template
        self.prompt_template = self._load_template(template) if template else None

        # Load system prompt
        self.system_prompt = self._load_system_prompt(system_prompt_file)

        # Tracking for truncation
        self.truncated_prompts_count = 0
        self.total_prompts_processed = 0

    def _load_template(self, template_type: str) -> str:
        """Load template from file based on template type"""
        if template_type in self.TEMPLATE_MAP:
            template_path = os.path.join(self.project_root, self.TEMPLATE_MAP[template_type])
        elif os.path.exists(template_type):
            template_path = template_type
        else:
            # Try absolute path from the original code
            abs_template_map = {
                "think": "/path/to/project/vulnerability_eval_pipeline/prompt_templates/think_templates.txt",
                "direct": "/path/to/project/vulnerability_eval_pipeline/prompt_templates/direct_templates.txt",
                "direct_new": "/path/to/project/vulnerability_eval_pipeline/prompt_templates/direct_new_templates.txt",
                "r2vul": "/path/to/project/vulnerability_eval_pipeline/prompt_templates/r2vul_user.txt",
                "think_not_tune": "/path/to/project/vulnerability_eval_pipeline/prompt_templates/think_templates_for_not_tune.txt",
                "think_with_cwe": "/path/to/project/vulnerability_eval_pipeline/prompt_templates/think_templates_withcwe.txt",
                "think_with_cwe_new": "/path/to/project/vulnerability_eval_pipeline/prompt_templates/think_templates_withcwe_new.txt"
            }
            if template_type in abs_template_map:
                template_path = abs_template_map[template_type]
            else:
                raise ValueError(f"Unknown template type: {template_type}. Must be one of {list(self.TEMPLATE_MAP.keys())}")

        try:
            with open(template_path, 'r', encoding='utf-8') as f:
                template = f.read().strip()
            self.logger.info(f"Loaded template from {template_path}")
            return template
        except FileNotFoundError:
            raise FileNotFoundError(f"Template file not found: {template_path}")
        except Exception as e:
            raise Exception(f"Error loading template from {template_path}: {e}")

    def _load_system_prompt(self, system_prompt_file: str = None) -> str:
        """Load system prompt from file"""
        if system_prompt_file:
            try:
                with open(system_prompt_file, 'r', encoding='utf-8') as f:
                    system_prompt = f.read().strip()
                self.logger.info(f"Loaded system prompt from {system_prompt_file}")
                return system_prompt
            except FileNotFoundError:
                self.logger.warning(f"System prompt file not found: {system_prompt_file}")
            except Exception as e:
                self.logger.warning(f"Error loading system prompt from {system_prompt_file}: {e}")

        self.logger.info("No system prompt specified")
        return ""

    def _load_api_key(self, provider: str) -> str:
        """
        Load API key from config file or environment variable.

        Args:
            provider: 'openai' or 'gemini'

        Returns:
            API key string
        """
        # Config file paths
        config_files = {
            'openai': os.path.join(self.project_root, 'config', 'openai_api_key.txt'),
            'gemini': os.path.join(self.project_root, 'config', 'gemini_api_key.txt')
        }

        # Environment variable names
        env_vars = {
            'openai': 'OPENAI_API_KEY',
            'gemini': 'GEMINI_API_KEY'
        }

        # Try config file first
        config_file = config_files.get(provider)
        if config_file and os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    api_key = f.read().strip()
                if api_key:
                    self.logger.info(f"Loaded {provider} API key from {config_file}")
                    return api_key
            except Exception as e:
                self.logger.warning(f"Error reading API key from {config_file}: {e}")

        # Try environment variable
        env_var = env_vars.get(provider)
        if env_var:
            api_key = os.environ.get(env_var)
            if api_key:
                self.logger.info(f"Loaded {provider} API key from environment variable {env_var}")
                return api_key

        raise ValueError(
            f"No API key found for {provider}. "
            f"Please set {env_var} environment variable or create {config_file}"
        )

    def load_data(self, data_path: str) -> List[Dict[str, Any]]:
        """Load data from JSONL file"""
        data = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    data.append(json.loads(line))
        return data

    def balanced_sample(self, data: List[Dict[str, Any]], max_samples: int, seed: int = 42) -> List[Dict[str, Any]]:
        """Create balanced sample of vulnerable and non-vulnerable examples"""
        np.random.seed(seed)

        vulnerable = [item for item in data if item.get('is_vulnerable', False)]
        not_vulnerable = [item for item in data if not item.get('is_vulnerable', True)]

        samples_per_class = max_samples // 2

        if len(vulnerable) >= samples_per_class and len(not_vulnerable) >= samples_per_class:
            sampled_vulnerable = np.random.choice(vulnerable, samples_per_class, replace=False).tolist()
            sampled_not_vulnerable = np.random.choice(not_vulnerable, samples_per_class, replace=False).tolist()
            return sampled_vulnerable + sampled_not_vulnerable
        else:
            self.logger.warning("Insufficient samples for balanced sampling, using available data")
            return data[:max_samples]

    def prepare_prompts_from_data(self, data: List[Dict[str, Any]], prompt_column: str = None) -> List[str]:
        """
        Prepare prompts from data items.

        Args:
            data: List of data items
            prompt_column: Optional column name containing pre-made prompts

        Returns:
            List of prompt strings
        """
        self.truncated_prompts_count = 0
        self.total_prompts_processed = len(data)

        specific_cwe_template = """\
Type: {cwe_id}: {cwe_name}
Description: {cwe_desc}
"""

        prompts = []
        for item in data:
            if prompt_column:
                # Use specified column
                if prompt_column in item:
                    prompt = item[prompt_column]
                else:
                    raise ValueError(f"Specified prompt column '{prompt_column}' not found in item: {list(item.keys())}")
            else:
                # Create prompt from code using template
                if self.prompt_template is None:
                    raise ValueError("No prompt template loaded. Please specify --template argument.")

                code = item.get('code')
                if code is None:
                    raise ValueError("'code' field not found in data item")
                language_name = item.get('language_name')
                if language_name is None:
                    raise ValueError("'language_name' field not found in data item")
                language_suffix = item.get('language_suffix')
                if language_suffix is None:
                    raise ValueError("'language_suffix' field not found in data item")
                cwe = item.get('cwe')
                if cwe is None:
                    raise ValueError("'cwe' field not found in data item")
                cwe_names = item.get('cwe_names')
                if cwe_names is None:
                    raise ValueError("'cwe_names' field not found in data item")
                cwe_descriptions = item.get('cwe_descriptions')
                if cwe_descriptions is None:
                    raise ValueError("'cwe_descriptions' field not found in data item")

                specific_cwe_desc = ""
                for id, name, desc in zip(cwe, cwe_names, cwe_descriptions):
                    specific_cwe_desc += specific_cwe_template.format(cwe_id=id, cwe_name=name, cwe_desc=desc) + "\n"

                prompt = self.prompt_template.format(
                    code=code,
                    language_name=language_name,
                    language_suffix=language_suffix,
                    specific_cwe_desc=specific_cwe_desc
                )

            prompts.append(prompt)

        return prompts

    def save_inference_results(self, prompts: List[str], responses: List[List[str]],
                                data: List[Dict[str, Any]], output_file: str,
                                inference_params: Dict[str, Any]) -> None:
        """Save inference results to JSONL file"""
        self.logger.info(f"Saving inference results to {output_file}")
        with open(output_file, 'w', encoding='utf-8') as f:
            for i, (prompt, response_list, metadata) in enumerate(zip(prompts, responses, data)):
                result_item = {
                    'index': i,
                    'prompt': prompt,
                    'responses': response_list,
                    'metadata': metadata,
                    'inference_params': inference_params
                }
                f.write(json.dumps(result_item) + '\n')

    @abstractmethod
    def run_inference(self, prompts: List[str], temperature: float = 0.0,
                      max_tokens: int = 4096, **kwargs) -> List[List[str]]:
        """
        Run inference on a list of prompts.

        Args:
            prompts: List of prompt strings
            temperature: Sampling temperature
            max_tokens: Maximum tokens to generate
            **kwargs: Additional backend-specific parameters

        Returns:
            List of response lists (one list per prompt for multi-sampling support)
        """
        pass

    @abstractmethod
    def run_inference_pipeline(self, data_path: str, output_dir: str,
                                temperature: float = 0.0, max_tokens: int = 4096,
                                max_samples: int = None, seed: int = 42,
                                balanced_sampling: bool = True,
                                prompt_column: str = None,
                                num_samples: int = 1, **kwargs) -> InferenceResult:
        """
        Run complete inference pipeline.

        Args:
            data_path: Path to input JSONL file
            output_dir: Directory for output files
            temperature: Sampling temperature
            max_tokens: Maximum tokens to generate
            max_samples: Maximum number of samples to process
            seed: Random seed
            balanced_sampling: Whether to use balanced sampling
            prompt_column: Column name containing prompts
            num_samples: Number of samples per prompt
            **kwargs: Additional backend-specific parameters

        Returns:
            InferenceResult with prompts, responses, and metadata
        """
        pass

    def get_output_filename(self, dataset_name: str, num_samples: int = 1) -> str:
        """Generate output filename based on model and template"""
        model_name = self.output_model_name or "model"
        template_name = self.template or "default"
        return f"{model_name}_{template_name}_{num_samples}_inference_results.jsonl"
