"""
OpenAI Batch API Inference Engine
Uses OpenAI's Batch API for cost-effective large-scale inference (50% discount)
"""

import json
import os
import time
from datetime import datetime
from typing import List, Dict, Any, Optional
from tqdm import tqdm

from openai import OpenAI

from .base import BaseInferenceEngine, InferenceResult


class OpenAIBatchEngine(BaseInferenceEngine):
    """Inference engine using OpenAI's Batch API"""

    def __init__(self, model_name: str = "gpt-4.1", template: str = None,
                 system_prompt_file: str = None, output_model_name: str = None,
                 max_prompt_tokens: int = None, api_key: str = None,
                 poll_interval: int = 30, max_wait_hours: int = 24,
                 batch_requests_dir: str = None, auto_submit: bool = False):
        """
        Initialize the OpenAI Batch API engine.

        Args:
            model_name: OpenAI model name (e.g., 'gpt-4.1', 'gpt-5.2')
            template: Template type or path
            system_prompt_file: Path to system prompt file
            output_model_name: Name to use in output files
            max_prompt_tokens: Maximum prompt length
            api_key: OpenAI API key (or read from config/env)
            poll_interval: Seconds between status polls
            max_wait_hours: Maximum hours to wait for batch completion
            batch_requests_dir: Directory to store batch request files
            auto_submit: Skip confirmation prompt and submit batch directly
        """
        # Initialize parent class first
        super().__init__(
            template=template,
            system_prompt_file=system_prompt_file,
            output_model_name=output_model_name or model_name,
            max_prompt_tokens=max_prompt_tokens
        )

        self.model_name = model_name
        self.poll_interval = poll_interval
        self.max_wait_hours = max_wait_hours
        self.auto_submit = auto_submit

        # Set batch requests directory
        if batch_requests_dir:
            self.batch_requests_dir = batch_requests_dir
        else:
            self.batch_requests_dir = os.path.join(self.project_root, 'batch_requests', 'openai')
        os.makedirs(self.batch_requests_dir, exist_ok=True)

        # Load API key
        if api_key:
            self._api_key = api_key
        else:
            self._api_key = self._load_api_key('openai')

        # Initialize OpenAI client
        self.client = OpenAI(api_key=self._api_key)
        self.logger.info(f"Initialized OpenAI Batch Engine with model: {model_name}")

    def _create_batch_request(self, prompt: str, custom_id: str,
                               temperature: float = None, max_tokens: int = None) -> Dict[str, Any]:
        """Create a single batch request entry"""
        messages = []

        # Add system message if available
        if self.system_prompt:
            messages.append({
                "role": "system",
                "content": self.system_prompt
            })

        # Add user message
        messages.append({
            "role": "user",
            "content": prompt
        })

        body = {
            "model": self.model_name,
            "messages": messages
        }

        # Only add temperature if specified
        if temperature is not None:
            body["temperature"] = temperature

        # Only add max_tokens if specified
        if max_tokens is not None:
            body["max_tokens"] = max_tokens

        return {
            "custom_id": custom_id,
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": body
        }

    def _create_batch_file(self, prompts: List[str], temperature: float,
                           max_tokens: int, dataset_name: str = None) -> str:
        """Create JSONL batch request file and return path"""
        # Generate filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        dataset_prefix = f"{dataset_name}_" if dataset_name else ""
        filename = f"{dataset_prefix}{self.model_name}_{timestamp}_requests.jsonl"
        batch_file_path = os.path.join(self.batch_requests_dir, filename)

        with open(batch_file_path, 'w', encoding='utf-8') as f:
            for i, prompt in enumerate(prompts):
                request = self._create_batch_request(
                    prompt=prompt,
                    custom_id=f"req-{i}",
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                f.write(json.dumps(request) + '\n')

        self.logger.info(f"Created batch file with {len(prompts)} requests: {batch_file_path}")
        return batch_file_path

    def _upload_batch_file(self, batch_file_path: str) -> str:
        """Upload batch file to OpenAI and return file ID"""
        self.logger.info(f"Uploading batch file to OpenAI: {batch_file_path}")

        with open(batch_file_path, 'rb') as f:
            batch_file = self.client.files.create(
                file=f,
                purpose="batch"
            )

        self.logger.info(f"Uploaded batch file with ID: {batch_file.id}")
        return batch_file.id

    def _create_batch_job(self, input_file_id: str) -> str:
        """Create batch job and return job ID"""
        self.logger.info("Creating batch job...")

        batch_job = self.client.batches.create(
            input_file_id=input_file_id,
            endpoint="/v1/chat/completions",
            completion_window="24h"
        )

        self.logger.info(f"Created batch job with ID: {batch_job.id}")
        return batch_job.id

    def _poll_batch_status(self, batch_job_id: str) -> Dict[str, Any]:
        """Poll batch job status until completion"""
        self.logger.info(f"Polling batch job status (interval: {self.poll_interval}s, max wait: {self.max_wait_hours}h)...")

        max_wait_seconds = self.max_wait_hours * 3600
        start_time = time.time()

        with tqdm(desc="Waiting for batch completion", unit="poll") as pbar:
            while True:
                batch_job = self.client.batches.retrieve(batch_job_id)
                status = batch_job.status

                pbar.set_postfix(status=status)
                pbar.update(1)

                if status == "completed":
                    self.logger.info("Batch job completed successfully!")
                    return {
                        'status': 'completed',
                        'output_file_id': batch_job.output_file_id,
                        'error_file_id': batch_job.error_file_id,
                        'request_counts': batch_job.request_counts
                    }
                elif status in ["failed", "expired", "cancelled"]:
                    error_msg = f"Batch job {status}"
                    if hasattr(batch_job, 'errors') and batch_job.errors:
                        error_msg += f": {batch_job.errors}"
                    self.logger.error(error_msg)
                    raise RuntimeError(error_msg)

                elapsed = time.time() - start_time
                if elapsed > max_wait_seconds:
                    raise TimeoutError(f"Batch job did not complete within {self.max_wait_hours} hours")

                time.sleep(self.poll_interval)

    def _download_results(self, output_file_id: str, batch_file_path: str = None) -> List[Dict[str, Any]]:
        """Download and parse batch results, optionally save to file"""
        self.logger.info("Downloading batch results...")

        result_content = self.client.files.content(output_file_id).content

        # Save results to file if batch_file_path provided
        if batch_file_path:
            results_file_path = batch_file_path.replace('_requests.jsonl', '_results.jsonl')
            with open(results_file_path, 'wb') as f:
                f.write(result_content)
            self.logger.info(f"Saved batch results to: {results_file_path}")

        # Parse JSONL results
        results = []
        for line in result_content.decode('utf-8').strip().split('\n'):
            if line:
                results.append(json.loads(line))

        self.logger.info(f"Downloaded {len(results)} results")
        return results

    def _parse_batch_results(self, results: List[Dict[str, Any]],
                              num_prompts: int) -> List[List[str]]:
        """Parse batch results into response format"""
        # Create mapping from custom_id to response
        response_map = {}
        for result in results:
            custom_id = result.get('custom_id', '')
            try:
                # Extract index from custom_id (format: "req-{index}")
                index = int(custom_id.split('-')[1])

                # Extract response content
                response_body = result.get('response', {}).get('body', {})
                choices = response_body.get('choices', [])

                if choices:
                    content = choices[0].get('message', {}).get('content', '')
                    response_map[index] = content
                else:
                    self.logger.warning(f"No choices in response for {custom_id}")
                    response_map[index] = ""

            except (ValueError, IndexError) as e:
                self.logger.warning(f"Error parsing result for {custom_id}: {e}")

        # Build ordered response list
        responses = []
        for i in range(num_prompts):
            if i in response_map:
                responses.append([response_map[i]])  # Wrap in list for consistency
            else:
                self.logger.warning(f"Missing response for index {i}")
                responses.append([""])

        return responses

    def run_inference(self, prompts: List[str], temperature: float = 0.0,
                      max_tokens: int = 4096, dataset_name: str = None,
                      **kwargs) -> List[List[str]]:
        """Run batch inference using OpenAI Batch API"""
        if not prompts:
            return []

        # Step 1: Create batch request file
        batch_file_path = self._create_batch_file(prompts, temperature, max_tokens, dataset_name)

        # Step 2: Ask user to confirm before submitting
        print(f"\n{'='*60}")
        print(f"Batch request file created: {batch_file_path}")
        print(f"Number of requests: {len(prompts)}")
        print(f"Model: {self.model_name}")
        temp_str = str(temperature) if temperature is not None else "API default"
        max_tokens_str = str(max_tokens) if max_tokens is not None else "API default"
        print(f"Temperature: {temp_str}, Max tokens: {max_tokens_str}")
        print(f"{'='*60}")

        # Show preview of first 3 requests
        print(f"\n--- Preview of first {min(3, len(prompts))} request(s) ---")
        with open(batch_file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= 3:
                    break
                req = json.loads(line)
                messages = req['body']['messages']
                user_msg = next((m['content'] for m in messages if m['role'] == 'user'), '')
                # Truncate long messages for display
                preview = user_msg
                print(f"\n[Request {i+1}]")
                print(preview)
        print(f"\n{'='*60}")

        # Skip confirmation if auto_submit is enabled
        if self.auto_submit:
            self.logger.info("Auto-submit enabled, proceeding with batch submission...")
            print("Auto-submit enabled, proceeding with batch submission...")
        else:
            confirm = input("Do you want to proceed with submitting this batch? (yes/no): ").strip().lower()
            if confirm not in ['yes', 'y']:
                self.logger.info("Batch submission cancelled by user.")
                print("Batch submission cancelled. You can review the file at:")
                print(f"  {batch_file_path}")
                return []

        # Step 3: Upload file to OpenAI
        input_file_id = self._upload_batch_file(batch_file_path)

        # Step 3: Create batch job
        batch_job_id = self._create_batch_job(input_file_id)

        # Step 4: Poll for completion
        job_result = self._poll_batch_status(batch_job_id)

        # Step 5: Download and parse results (also saves to file)
        results = self._download_results(job_result['output_file_id'], batch_file_path)

        # Step 6: Parse into response format
        responses = self._parse_batch_results(results, len(prompts))

        return responses

    def run_inference_pipeline(self, data_path: str, output_dir: str,
                                temperature: float = 0.0, max_tokens: int = 4096,
                                max_samples: int = None, seed: int = 42,
                                balanced_sampling: bool = True,
                                prompt_column: str = None,
                                num_samples: int = 1, **kwargs) -> InferenceResult:
        """Run complete inference pipeline using OpenAI Batch API"""
        # Extract dataset name and create output directory
        dataset_name = os.path.splitext(os.path.basename(data_path))[0]
        dataset_output_dir = os.path.join(output_dir, dataset_name)
        os.makedirs(dataset_output_dir, exist_ok=True)

        # Load data
        self.logger.info(f"Loading data from {data_path}")
        data = self.load_data(data_path)

        if max_samples:
            if balanced_sampling:
                data = self.balanced_sample(data, max_samples, seed)
                self.logger.info(f"Using balanced sample of {len(data)} samples for inference")
            else:
                data = data[:max_samples]
                self.logger.info(f"Using first {len(data)} samples for inference")

        # Prepare prompts
        prompts = self.prepare_prompts_from_data(data, prompt_column)
        self.logger.info(f"Prepared {len(prompts)} prompts")

        # Run batch inference
        self.logger.info("Starting OpenAI Batch API inference...")
        responses = self.run_inference(prompts, temperature, max_tokens, dataset_name=dataset_name)

        # Create result
        result = InferenceResult(
            prompts=prompts,
            responses=responses,
            metadata=data
        )

        # Save results
        model_name = self.output_model_name or self.model_name
        inference_file = os.path.join(
            dataset_output_dir,
            f"{model_name}_{self.template}_{num_samples}_inference_results.jsonl"
        )

        inference_params = {
            'temperature': temperature,
            'max_tokens': max_tokens,
            'seed': seed,
            'num_samples': num_samples,
            'backend': 'openai',
            'model': self.model_name
        }

        self.save_inference_results(prompts, responses, data, inference_file, inference_params)
        self.logger.info(f"Inference completed. Results saved to {inference_file}")

        return result
