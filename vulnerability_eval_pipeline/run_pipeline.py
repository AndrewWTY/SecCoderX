#!/usr/bin/env python3
"""
Complete Vulnerability Detection Pipeline
Orchestrates inference and evaluation steps with multiple backend support
Supports: vLLM (local models), OpenAI Batch API, Gemini Batch API
"""

import argparse
import os
import logging

from inference_engines import get_engine
from vulnerability_evaluator import VulnerabilityEvaluator


def main():
    parser = argparse.ArgumentParser(
        description="Run vulnerability detection pipeline with multiple inference backends",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # vLLM (local model)
  python run_pipeline.py --backend vllm --model_path /path/to/model --data_path data.jsonl --template direct

  # OpenAI Batch API
  python run_pipeline.py --backend openai --model_name gpt-4.1 --data_path data.jsonl --template direct

  # Gemini Batch API
  python run_pipeline.py --backend gemini --model_name gemini-2.5-flash-pro --data_path data.jsonl --template direct

  # CodeQL Static Analysis (all supported languages)
  python run_pipeline.py --backend codeql --data_path data.jsonl

  # Bandit Static Analysis (Python only)
  python run_pipeline.py --backend bandit --data_path data.jsonl

  # Semgrep Static Analysis (multi-language)
  python run_pipeline.py --backend semgrep --data_path data.jsonl
        """
    )

    # Backend selection
    parser.add_argument("--backend", type=str, default="vllm",
                        choices=["vllm", "openai", "gemini", "codeql", "bandit", "semgrep"],
                        help="Inference backend: vllm (local), openai (Batch API), gemini (Batch API), codeql (static analysis), bandit (Python static analysis), semgrep (multi-language static analysis)")

    # Model specification
    parser.add_argument("--model_path", type=str, default=None,
                        help="Path to local model (required for vllm backend)")
    parser.add_argument("--model_name", type=str, default=None,
                        help="Model name for API backends (e.g., 'gpt-4.1', 'gemini-2.5-flash-pro')")
    parser.add_argument("--output_model_name", type=str, default=None,
                        help="Custom name for output files (default: auto-generated)")

    # Data path
    parser.add_argument("--data_path", type=str, required=True,
                        help="Path to the data file (JSONL)")

    # Output directories
    parser.add_argument("--inference_dir", type=str, default="./inference_results",
                        help="Output directory for inference results")
    parser.add_argument("--eval_dir", type=str, default="./eval_results",
                        help="Output directory for evaluation results")

    # Template configuration
    parser.add_argument("--template", type=str, required=False, default=None,
                        choices=["think", "direct", "direct_new", "r2vul",
                                 "think_not_tune", "think_with_cwe", "think_with_cwe_new", "think_with_cwe_new_blackbox"],
                        help="Template type for prompt formatting (not required for codeql backend)")
    parser.add_argument("--system_prompt", type=str, default=None,
                        help="Path to system prompt file")
    parser.add_argument("--prompt_column", type=str, default=None,
                        help="Column name containing pre-made prompts")

    # Inference parameters
    parser.add_argument("--temperature", type=float, default=None,
                        help="Sampling temperature (default: API default)")
    parser.add_argument("--max_tokens", type=int, default=None,
                        help="Maximum tokens to generate (default: API default)")
    parser.add_argument("--max_samples", type=int, default=None,
                        help="Maximum number of samples to process")
    parser.add_argument("--limit", type=int, default=None,
                        help="Limit samples for quick testing (alias for --max_samples)")
    parser.add_argument("--seed", type=int, default=42,
                        help="Random seed")
    parser.add_argument("--num_samples", type=int, default=1,
                        help="Number of samples per prompt (for majority voting)")

    # vLLM-specific parameters
    parser.add_argument("--tensor_parallel_size", type=int, default=1,
                        help="Number of GPUs for tensor parallelism (vllm only)")
    parser.add_argument("--gpu_memory_utilization", type=float, default=0.8,
                        help="GPU memory utilization ratio (vllm only)")
    parser.add_argument("--max_model_len", type=int, default=None,
                        help="Maximum model context length (vllm only)")
    parser.add_argument("--max_prompt_tokens", type=int, default=None,
                        help="Maximum prompt length before truncation")
    parser.add_argument("--format_prompt", action="store_true", default=True,
                        help="Use chat template formatting (vllm only)")
    parser.add_argument("--top_p", type=float, default=None,
                        help="Top-p sampling parameter (vllm only)")
    parser.add_argument("--top_k", type=int, default=None,
                        help="Top-k sampling parameter (vllm only)")

    # API-specific parameters
    parser.add_argument("--api_key", type=str, default=None,
                        help="API key (overrides config file and environment variable)")
    parser.add_argument("--poll_interval", type=int, default=30,
                        help="Seconds between batch status polls (API backends)")
    parser.add_argument("--max_wait_hours", type=int, default=24,
                        help="Maximum hours to wait for batch completion (API backends)")
    parser.add_argument("--auto_submit", action="store_true",
                        help="Skip confirmation prompt and submit batch directly (API backends)")

    # CodeQL-specific parameters
    parser.add_argument("--codeql_path", type=str, default=None,
                        help="Path to codeql CLI binary (auto-detect if not specified)")
    parser.add_argument("--codeql_threads", type=int, default=4,
                        help="Number of parallel CodeQL analysis threads")
    parser.add_argument("--codeql_timeout", type=int, default=300,
                        help="Timeout per code snippet in seconds")

    # Bandit-specific parameters
    parser.add_argument("--bandit_path", type=str, default=None,
                        help="Path to bandit CLI binary (auto-detect if not specified)")
    parser.add_argument("--bandit_threads", type=int, default=8,
                        help="Number of parallel Bandit analysis threads")
    parser.add_argument("--bandit_timeout", type=int, default=60,
                        help="Timeout per code snippet in seconds")

    # Semgrep-specific parameters
    parser.add_argument("--semgrep_path", type=str, default=None,
                        help="Path to semgrep CLI binary (auto-detect if not specified)")
    parser.add_argument("--semgrep_threads", type=int, default=8,
                        help="Number of parallel Semgrep analysis threads")
    parser.add_argument("--semgrep_timeout", type=int, default=120,
                        help="Timeout per code snippet in seconds")
    parser.add_argument("--semgrep_config", type=str, default="p/security-audit",
                        help="Semgrep ruleset config (default: p/security-audit)")

    # Sampling configuration
    parser.add_argument("--no_balanced_sampling", action="store_true",
                        help="Disable balanced sampling")

    # Pipeline control
    parser.add_argument("--inference_only", action="store_true",
                        help="Run inference only, skip evaluation")
    parser.add_argument("--eval_only", action="store_true",
                        help="Run evaluation only, skip inference")
    parser.add_argument("--inference_file", type=str, default=None,
                        help="Path to existing inference results file (for eval_only mode)")

    # Evaluation parameters
    parser.add_argument("--is_think_response", action="store_true", default=True,
                        help="Parse responses with <answer> tags (enabled by default)")
    parser.add_argument("--no_think_response", action="store_true",
                        help="Disable <answer> tag parsing, use [[YES]]/[[NO]] only")

    args = parser.parse_args()

    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger(__name__)

    # Validate arguments
    if args.eval_only and not args.inference_file:
        parser.error("--inference_file is required when using --eval_only")

    if args.inference_only and args.eval_only:
        parser.error("Cannot use both --inference_only and --eval_only")

    if args.backend == "vllm" and not args.model_path:
        parser.error("--model_path is required for vllm backend")

    if args.backend in ["openai", "gemini"] and not args.model_name:
        parser.error(f"--model_name is required for {args.backend} backend")

    # Template is required for LLM backends but not for static analysis backends
    if args.backend not in ["codeql", "bandit", "semgrep"] and not args.template:
        parser.error("--template is required for vllm, openai, and gemini backends")

    # Handle --limit as alias for --max_samples
    if args.limit is not None:
        if args.max_samples is not None:
            logger.warning("Both --limit and --max_samples specified, using --limit")
        args.max_samples = args.limit

    # Step 1: Run inference (unless eval_only)
    inference_file = None
    if not args.eval_only:
        logger.info(f"=== Running Inference Step (backend: {args.backend}) ===")

        # Build engine kwargs based on backend
        engine_kwargs = {
            'template': args.template,
            'system_prompt_file': args.system_prompt,
            'output_model_name': args.output_model_name,
            'max_prompt_tokens': args.max_prompt_tokens,
        }

        if args.backend == "vllm":
            engine_kwargs.update({
                'model_path': args.model_path,
                'tensor_parallel_size': args.tensor_parallel_size,
                'gpu_memory_utilization': args.gpu_memory_utilization,
                'max_model_len': args.max_model_len,
            })
        elif args.backend == "openai":
            engine_kwargs.update({
                'model_name': args.model_name,
                'api_key': args.api_key,
                'poll_interval': args.poll_interval,
                'max_wait_hours': args.max_wait_hours,
                'auto_submit': args.auto_submit,
            })
        elif args.backend == "gemini":
            engine_kwargs.update({
                'model_name': args.model_name,
                'api_key': args.api_key,
                'poll_interval': args.poll_interval,
                'max_wait_hours': args.max_wait_hours,
                'auto_submit': args.auto_submit,
            })
        elif args.backend == "codeql":
            engine_kwargs.update({
                'codeql_path': args.codeql_path,
                'threads': args.codeql_threads,
                'timeout': args.codeql_timeout,
            })
        elif args.backend == "bandit":
            engine_kwargs.update({
                'bandit_path': args.bandit_path,
                'threads': args.bandit_threads,
                'timeout': args.bandit_timeout,
            })
        elif args.backend == "semgrep":
            engine_kwargs.update({
                'semgrep_path': args.semgrep_path,
                'threads': args.semgrep_threads,
                'timeout': args.semgrep_timeout,
                'config': args.semgrep_config,
            })

        # Initialize engine using factory
        engine = get_engine(args.backend, **engine_kwargs)

        # Build inference pipeline kwargs
        pipeline_kwargs = {
            'data_path': args.data_path,
            'output_dir': args.inference_dir,
            'temperature': args.temperature,
            'max_tokens': args.max_tokens,
            'max_samples': args.max_samples,
            'seed': args.seed,
            'balanced_sampling': not args.no_balanced_sampling,
            'prompt_column': args.prompt_column,
            'num_samples': args.num_samples,
        }

        # Add vLLM-specific parameters
        if args.backend == "vllm":
            pipeline_kwargs.update({
                'format_prompt': args.format_prompt,
                'top_p': args.top_p,
                'top_k': args.top_k,
            })

        # Run inference
        result = engine.run_inference_pipeline(**pipeline_kwargs)

        # Determine inference file path
        dataset_name = os.path.splitext(os.path.basename(args.data_path))[0]
        if args.output_model_name:
            model_name = args.output_model_name
        elif args.backend == "vllm":
            model_name = os.path.basename(args.model_path).replace('/', '_')
        elif args.backend == "codeql":
            model_name = "codeql-security"
        elif args.backend == "bandit":
            model_name = "bandit-security"
        elif args.backend == "semgrep":
            model_name = "semgrep-security"
        else:
            model_name = args.model_name

        # For static analysis backends, use backend name as template; for others use args.template
        if args.backend == "codeql":
            template_name = "codeql"
        elif args.backend == "bandit":
            template_name = "bandit"
        elif args.backend == "semgrep":
            template_name = "semgrep"
        else:
            template_name = args.template
        inference_file = os.path.join(
            args.inference_dir, dataset_name,
            f"{model_name}_{template_name}_{args.num_samples}_inference_results.jsonl"
        )

        logger.info(f"Inference completed. Results saved to {inference_file}")

        if args.inference_only:
            logger.info("Inference-only mode. Skipping evaluation.")
            return
    else:
        inference_file = args.inference_file
        logger.info(f"Using existing inference results from {inference_file}")

    # Step 2: Run evaluation (unless inference_only)
    if not args.inference_only:
        logger.info("=== Running Evaluation Step ===")

        # Initialize evaluator (is_think_response enabled by default, disable with --no_think_response)
        is_think = args.is_think_response and not args.no_think_response
        evaluator = VulnerabilityEvaluator(is_think_response=is_think)

        # Run evaluation
        eval_result = evaluator.run_evaluation_pipeline(
            inference_file=inference_file,
            output_dir=args.eval_dir
        )

        logger.info("Evaluation completed successfully!")

    logger.info("Pipeline completed successfully!")


if __name__ == "__main__":
    main()
